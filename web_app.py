# ·ª®ng d·ª•ng Web cho Tr·ª£ L√Ω B√°o C√°o Chi Ph√≠
"""
·ª®ng d·ª•ng web Flask cho chatbot tr·ª£ l√Ω b√°o c√°o chi ph√≠
v·ªõi giao di·ªán th√¢n thi·ªán v√† t√≠nh nƒÉng chat th·ªùi gian th·ª±c.
"""

import asyncio
import concurrent.futures
import json
import os
import threading
import time
import uuid
from collections import defaultdict
from datetime import datetime
from queue import Empty, Queue

import torch
from flask import Flask, jsonify, render_template, request, send_file, session
from flask_cors import CORS
from transformers import AutoTokenizer, VitsModel

from cli import run_batch_chat, run_expense_batch_processing
from database import ExpenseDB
from expense_assistant import ExpenseAssistant, create_client
from functions import EXPENSE_POLICIES, MOCK_EXPENSE_REPORTS, SAMPLE_USER_QUERIES
from text_to_speech import text_to_speech as tts

app = Flask(__name__)
app.secret_key = "expense_assistant_secret_key_2024"
CORS(app)

# Kh·ªüi t·∫°o c∆° s·ªü d·ªØ li·ªáu
db = ExpenseDB()

# üÜï BATCHING SYSTEM FOR MULTIPLE USERS
class BatchingQueue:
    """H·ªá th·ªëng batching t·ª± ƒë·ªông cho multiple users"""

    def __init__(self, batch_size=5, max_wait_time=2.0):
        self.batch_size = batch_size  # S·ªë requests t·ªëi ƒëa trong 1 batch
        self.max_wait_time = max_wait_time  # Th·ªùi gian ch·ªù t·ªëi ƒëa (gi√¢y)
        self.pending_requests = Queue()
        self.response_futures = {}
        self.is_running = False
        self.batch_thread = None
        self.stats = {
            "total_requests": 0,
            "total_batches": 0,
            "total_tokens_saved": 0,
            "average_batch_size": 0,
        }

    def start(self):
        """Kh·ªüi ƒë·ªông batch processing thread"""
        if not self.is_running:
            self.is_running = True
            self.batch_thread = threading.Thread(
                target=self._batch_processor, daemon=True
            )
            self.batch_thread.start()
            print("üöÄ Batch processing system started")

    def stop(self):
        """D·ª´ng batch processing thread"""
        self.is_running = False
        if self.batch_thread:
            self.batch_thread.join()
            print("‚èπÔ∏è Batch processing system stopped")

    def add_request(self, session_id, message, assistant):
        """Th√™m request v√†o queue ƒë·ªÉ batch processing"""
        request_id = str(uuid.uuid4())
        future = concurrent.futures.Future()

        request_data = {
            "id": request_id,
            "session_id": session_id,
            "message": message,
            "assistant": assistant,
            "timestamp": time.time(),
            "future": future,
        }

        self.pending_requests.put(request_data)
        self.response_futures[request_id] = future
        self.stats["total_requests"] += 1

        return future

    def _batch_processor(self):
        """Main batch processing loop"""
        while self.is_running:
            try:
                batch = self._collect_batch()
                if batch:
                    self._process_batch(batch)
                else:
                    time.sleep(0.1)  # Short sleep if no requests
            except Exception as e:
                print(f"‚ùå Batch processor error: {e}")
                time.sleep(1)

    def _collect_batch(self):
        """Thu th·∫≠p requests ƒë·ªÉ t·∫°o batch"""
        batch = []
        deadline = time.time() + self.max_wait_time

        # L·∫•y request ƒë·∫ßu ti√™n (blocking v·ªõi timeout)
        try:
            first_request = self.pending_requests.get(timeout=1.0)
            batch.append(first_request)
        except Empty:
            return None

        # Thu th·∫≠p th√™m requests cho ƒë·∫øn khi ƒë·∫°t batch_size ho·∫∑c timeout
        while len(batch) < self.batch_size and time.time() < deadline:
            try:
                remaining_time = max(0, deadline - time.time())
                request = self.pending_requests.get(timeout=remaining_time)
                batch.append(request)
            except Empty:
                break

        return batch

    def _process_batch(self, batch):
        """X·ª≠ l√Ω m·ªôt batch requests"""
        if not batch:
            return

        self.stats["total_batches"] += 1
        batch_size = len(batch)
        self.stats["average_batch_size"] = (
            self.stats["average_batch_size"] * (self.stats["total_batches"] - 1)
            + batch_size
        ) / self.stats["total_batches"]

        print(f"üîÑ Processing batch of {batch_size} requests")

        try:
            # N·∫øu ch·ªâ c√≥ 1 request, x·ª≠ l√Ω b√¨nh th∆∞·ªùng
            if batch_size == 1:
                self._process_single_request(batch[0])
            else:
                # Batch processing cho multiple requests
                self._process_multiple_requests(batch)

        except Exception as e:
            print(f"‚ùå Batch processing error: {e}")
            # Set error cho t·∫•t c·∫£ requests trong batch
            for req in batch:
                req["future"].set_exception(e)
                if req["id"] in self.response_futures:
                    del self.response_futures[req["id"]]

    def _process_single_request(self, request):
        """X·ª≠ l√Ω single request"""
        try:
            assistant = request["assistant"]
            response = assistant.get_response(request["message"])

            request["future"].set_result(
                {
                    "success": True,
                    "response": response["content"],
                    "tokens_used": response.get("total_tokens", 0),
                    "function_calls": len(response.get("tool_calls", [])),
                    "function_details": response.get("tool_calls", []),
                    "batch_size": 1,
                    "batch_processing": False,
                }
            )

        except Exception as e:
            request["future"].set_exception(e)
        finally:
            if request["id"] in self.response_futures:
                del self.response_futures[request["id"]]

    def _process_multiple_requests(self, batch):
        """X·ª≠ l√Ω multiple requests v·ªõi batching optimization"""
        try:
            # Gom t·∫•t c·∫£ messages
            messages = [req["message"] for req in batch]

            # S·ª≠ d·ª•ng assistant t·ª´ request ƒë·∫ßu ti√™n (c√≥ th·ªÉ optimize th√™m)
            primary_assistant = batch[0]["assistant"]

            # Batch processing
            batch_query = f"X·ª≠ l√Ω batch {len(messages)} c√¢u h·ªèi:\n"
            for i, msg in enumerate(messages, 1):
                batch_query += f"{i}. {msg}\n"

            batch_query += "\nVui l√≤ng tr·∫£ l·ªùi t·ª´ng c√¢u h·ªèi m·ªôt c√°ch ng·∫Øn g·ªçn v√† c√≥ s·ªë th·ª© t·ª± t∆∞∆°ng ·ª©ng."

            # G·ªçi API m·ªôt l·∫ßn cho c·∫£ batch
            response = primary_assistant.get_response(batch_query)

            # Parse response ƒë·ªÉ chia cho t·ª´ng request
            responses = self._parse_batch_response(response["content"], len(batch))

            # Estimate token savings
            estimated_single_tokens = len(batch) * 150  # Estimate
            actual_tokens = response.get("total_tokens", 0)
            tokens_saved = max(0, estimated_single_tokens - actual_tokens)
            self.stats["total_tokens_saved"] += tokens_saved

            # Set results cho t·ª´ng request
            for i, req in enumerate(batch):
                individual_response = (
                    responses[i] if i < len(responses) else "L·ªói x·ª≠ l√Ω batch response"
                )

                req["future"].set_result(
                    {
                        "success": True,
                        "response": individual_response,
                        "tokens_used": actual_tokens // len(batch),  # Chia ƒë·ªÅu tokens
                        "function_calls": len(response.get("tool_calls", [])),
                        "function_details": response.get("tool_calls", []),
                        "batch_size": len(batch),
                        "batch_processing": True,
                        "tokens_saved": tokens_saved // len(batch),
                    }
                )

                if req["id"] in self.response_futures:
                    del self.response_futures[req["id"]]

        except Exception as e:
            # Set error cho t·∫•t c·∫£ requests
            for req in batch:
                req["future"].set_exception(e)
                if req["id"] in self.response_futures:
                    del self.response_futures[req["id"]]

    def _parse_batch_response(self, batch_response, expected_count):
        """Parse batch response th√†nh individual responses"""
        responses = []

        # Try to split by numbers (1., 2., 3., etc.)
        import re

        parts = re.split(r"\n?\d+\.\s*", batch_response)

        # Remove empty first part if exists
        if parts and not parts[0].strip():
            parts = parts[1:]

        # If we don't get expected count, try other splitting methods
        if len(parts) != expected_count:
            # Try splitting by lines
            lines = [
                line.strip() for line in batch_response.split("\n") if line.strip()
            ]
            if len(lines) >= expected_count:
                parts = lines[:expected_count]
            else:
                # Fallback: repeat the whole response
                parts = [batch_response] * expected_count

        # Ensure we have enough responses
        while len(parts) < expected_count:
            parts.append("Kh√¥ng th·ªÉ x·ª≠ l√Ω c√¢u h·ªèi n√†y trong batch.")

        return parts[:expected_count]

    def get_stats(self):
        """L·∫•y th·ªëng k√™ batching"""
        return self.stats.copy()


# Kh·ªüi t·∫°o batching system
batching_queue = BatchingQueue(batch_size=5, max_wait_time=2.0)

# üîß BATCHING CONFIGURATION
ENABLE_AUTO_BATCHING = False  # B·∫≠t/t·∫Øt auto batching
BATCHING_CONFIG = {
    "batch_size": 5,  # S·ªë requests t·ªëi ƒëa trong 1 batch
    "max_wait_time": 2.0,  # Th·ªùi gian ch·ªù t·ªëi ƒëa (gi√¢y)
    "min_batch_size": 2,  # S·ªë requests t·ªëi thi·ªÉu ƒë·ªÉ trigger batch
}


# Kh·ªüi t·∫°o assistant
try:
    client = create_client()
    assistant = ExpenseAssistant(client, model="GPT-4o-mini")
except Exception as e:
    print(f"L·ªói kh·ªüi t·∫°o assistant: {e}")
    assistant = None

# Dictionary ƒë·ªÉ l∆∞u tr·ªØ c√°c phi√™n chat
chat_sessions = {}


@app.route("/")
def home():
    """Trang ch·ªß c·ªßa ·ª©ng d·ª•ng web."""
    return render_template("index.html")


@app.route("/api/start_session", methods=["POST"])
def start_session():
    """B·∫Øt ƒë·∫ßu m·ªôt phi√™n chat m·ªõi."""
    session_id = str(uuid.uuid4())

    # T·∫°o assistant m·ªõi cho phi√™n n√†y
    if assistant:
        try:
            session_client = create_client()
            session_assistant = ExpenseAssistant(session_client, model="GPT-4o-mini")
            chat_sessions[session_id] = {
                "assistant": session_assistant,
                "created_at": datetime.now().isoformat(),
                "message_count": 0,
            }

            return jsonify(
                {
                    "success": True,
                    "session_id": session_id,
                    "message": "Phi√™n chat ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!",
                }
            )
        except Exception as e:
            return (
                jsonify({"success": False, "error": f"L·ªói t·∫°o phi√™n chat: {str(e)}"}),
                500,
            )
    else:
        return jsonify({"success": False, "error": "Assistant ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o"}), 500


@app.route("/api/chat", methods=["POST"])
def chat():
    """X·ª≠ l√Ω tin nh·∫Øn chat v·ªõi auto-batching."""
    data = request.get_json()
    session_id = data.get("session_id")
    message = data.get("message", "").strip()

    if not session_id or session_id not in chat_sessions:
        return jsonify({"success": False, "error": "Phi√™n chat kh√¥ng h·ª£p l·ªá"}), 400

    if not message:
        return jsonify({"success": False, "error": "Tin nh·∫Øn kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng"}), 400

    try:
        session_assistant = chat_sessions[session_id]["assistant"]

        # B∆∞·ªõc 1: T√¨m ki·∫øm trong imported data t·ª´ setup_db.py
        try:
            imported_collection = db.client.get_collection("imported_data")
            if imported_collection:
                # T√¨m ki·∫øm trong d·ªØ li·ªáu ƒë√£ import
                import_results = imported_collection.query(
                    query_texts=[message],
                    n_results=2,  # L·∫•y 2 k·∫øt qu·∫£ ph√π h·ª£p nh·∫•t
                    where={"type": {"$in": ["policy", "expense_rule", "guideline"]}}  # L·ªçc theo lo·∫°i d·ªØ li·ªáu
                )
                
                # N·∫øu t√¨m th·∫•y k·∫øt qu·∫£ ph√π h·ª£p trong imported data
                if import_results and import_results['distances'][0] and min(import_results['distances'][0]) < 0.25:
                    imported_contexts = []
                    for i, doc in enumerate(import_results['documents'][0]):
                        meta = import_results['metadatas'][0][i]
                        imported_contexts.append(f"Th√¥ng tin t·ª´ {meta['type']}: {doc}")
                    
                    if imported_contexts:
                        message = f"Context t·ª´ d·ªØ li·ªáu c√≥ s·∫µn:\n{chr(10).join(imported_contexts)}\n\nC√¢u h·ªèi: {message}"
        except Exception as e:
            print(f"Warning: Kh√¥ng th·ªÉ t√¨m trong imported data: {str(e)}")

        # B∆∞·ªõc 2: T√¨m ki·∫øm trong l·ªãch s·ª≠ chat
        chat_history_collection = db.client.get_collection(name="chat_history")
        if chat_history_collection:
            # T√¨m ki·∫øm c√°c tin nh·∫Øn t∆∞∆°ng t·ª±
            results = chat_history_collection.query(
                query_texts=[message],
                n_results=3,  # L·∫•y 3 k·∫øt qu·∫£ t∆∞∆°ng t·ª± nh·∫•t
                where={"session_id": session_id}  # L·ªçc theo session hi·ªán t·∫°i
            )
            
            # N·∫øu t√¨m th·∫•y k·∫øt qu·∫£ t∆∞∆°ng t·ª±
            if results and results['distances'][0] and min(results['distances'][0]) < 0.3:  # Ng∆∞·ª°ng t∆∞∆°ng ƒë·ªìng
                similar_responses = []
                for i, doc in enumerate(results['documents'][0]):
                    if results['metadatas'][0][i]['role'] == 'assistant':
                        similar_responses.append(doc)
                
                # Th√™m context t·ª´ l·ªãch s·ª≠ chat
                if similar_responses:
                    chat_context = "\n".join([f"C√¢u tr·∫£ l·ªùi t∆∞∆°ng t·ª± tr∆∞·ªõc ƒë√¢y: {resp}" for resp in similar_responses])
                    if "Context t·ª´ d·ªØ li·ªáu c√≥ s·∫µn:" in message:
                        message += f"\n\nContext t·ª´ l·ªãch s·ª≠ chat:\n{chat_context}"
                    else:
                        message = f"Context t·ª´ l·ªãch s·ª≠ chat:\n{chat_context}\n\nC√¢u h·ªèi hi·ªán t·∫°i: {message}"

        # G·ª≠i tin nh·∫Øn ƒë·∫øn assistant
        response = session_assistant.get_response(message)

        # C·∫≠p nh·∫≠t s·ªë l∆∞·ª£ng tin nh·∫Øn
        chat_sessions[session_id]["message_count"] += 1

        # --- L∆∞u l·ªãch s·ª≠ chat v√†o ChromaDB ---
        chat_history_collection = db.client.get_or_create_collection(
            name="chat_history",
            embedding_function=db.embedding_fn,
            metadata={"description": "L·ªãch s·ª≠ chat c·ªßa user v√† assistant"},
        )
        chat_history_collection.add(
            documents=[message],
            ids=[f"{session_id}_user_{chat_sessions[session_id]['message_count']}"],
            metadatas=[{"session_id": session_id, "role": "user"}],
        )
        chat_history_collection.add(
            documents=[response["content"]],
            ids=[
                f"{session_id}_assistant_{chat_sessions[session_id]['message_count']}"
            ],
            metadatas=[{"session_id": session_id, "role": "assistant"}],
        )
        # --- K·∫øt th√∫c l∆∞u l·ªãch s·ª≠ ---

        # --- T·∫°o v√† l∆∞u summary v√†o ChromaDB ---
        # L·∫•y summary (gi·∫£ s·ª≠ assistant c√≥ h√†m get_conversation_summary)
        summary = session_assistant.get_conversation_summary()
        chat_summary_collection = db.client.get_or_create_collection(
            name="chat_summaries",
            embedding_function=db.embedding_fn,
            metadata={"description": "T√≥m t·∫Øt h·ªôi tho·∫°i theo session"},
        )
        chat_summary_collection.add(
            documents=[summary],
            ids=[f"{session_id}_summary"],
            metadatas=[{"session_id": session_id, "type": "summary"}],
        )
        # --- K·∫øt th√∫c l∆∞u summary ---

        # üÜï S·ª¨ D·ª§NG BATCHING SYSTEM
        if ENABLE_AUTO_BATCHING:
            # Th√™m request v√†o batching queue
            future = batching_queue.add_request(session_id, message, session_assistant)

            # Ch·ªù k·∫øt qu·∫£ t·ª´ batch processing
            result = future.result(timeout=10.0)  # 10s timeout

            # C·∫≠p nh·∫≠t message count
            chat_sessions[session_id]["message_count"] += 1

            return jsonify(result)

        else:
            # X·ª≠ l√Ω b√¨nh th∆∞·ªùng (kh√¥ng batching)
            response = session_assistant.get_response(message)

            # C·∫≠p nh·∫≠t s·ªë l∆∞·ª£ng tin nh·∫Øn
            chat_sessions[session_id]["message_count"] += 1
            print(response["content"])
            return jsonify(
                {
                    "success": True,
                    "response": response["content"],
                    "function_calls": len(response.get("tool_calls", [])),
                    "tokens_used": response.get("total_tokens", 0),
                    "has_function_calls": len(response.get("tool_calls", [])) > 0,
                    "function_details": response.get("tool_calls", []),
                    "batch_processing": False,
                    "batch_size": 1,
                }
            )

    except concurrent.futures.TimeoutError:
        return (
            jsonify({"success": False, "error": "Timeout - Y√™u c·∫ßu x·ª≠ l√Ω qu√° l√¢u"}),
            504,
        )

    except Exception as e:
        return (
            jsonify({"success": False, "error": f"L·ªói x·ª≠ l√Ω tin nh·∫Øn: {str(e)}"}),
            500,
        )


@app.route("/api/text-to-speech", methods=["POST"])
def text_to_speech_route():
    """Converts text to speech and returns the audio file."""
    data = request.get_json()
    text = data.get("text", "").strip()

    if not text:
        return jsonify({"success": False, "error": "Text cannot be empty"}), 400

    try:
        output_filename = f"speech_{uuid.uuid4()}.wav"
        output_path = tts(text, output_filename)
        audio_url = f"/audio/{output_filename}"

        return jsonify({"success": True, "audio_url": audio_url})

    except Exception as e:
        return (
            jsonify({"success": False, "error": f"Error generating speech: {str(e)}"}),
            500,
        )


@app.route("/audio/<filename>")
def serve_audio(filename):
    """Serves the generated audio file."""
    return send_file(os.path.join("audio_chats", filename))


@app.route("/api/batch_chat", methods=["POST"])
def batch_chat():
    """X·ª≠ l√Ω batch chat v·ªõi nhi·ªÅu queries c√πng l√∫c."""
    data = request.get_json()
    session_id = data.get("session_id")
    queries = data.get("queries", [])
    batch_size = data.get("batch_size", 3)

    if not session_id or session_id not in chat_sessions:
        return jsonify({"success": False, "error": "Phi√™n chat kh√¥ng h·ª£p l·ªá"}), 400

    if not queries or not isinstance(queries, list):
        return (
            jsonify({"success": False, "error": "Danh s√°ch queries kh√¥ng h·ª£p l·ªá"}),
            400,
        )

    try:
        session_assistant = chat_sessions[session_id]["assistant"]

        # Th·ª±c hi·ªán batch processing
        start_time = datetime.now()
        batch_results = []

        # Chia queries th√†nh c√°c batch nh·ªè
        for i in range(0, len(queries), batch_size):
            batch_queries = queries[i : i + batch_size]

            # X·ª≠ l√Ω parallel trong batch
            batch_responses = []
            for query in batch_queries:
                try:
                    response = session_assistant.get_response(query.strip())
                    batch_responses.append(
                        {
                            "query": query,
                            "response": response.get("content", ""),
                            "tokens_used": response.get("total_tokens", 0),
                            "function_calls": len(response.get("tool_calls", [])),
                            "success": True,
                        }
                    )
                except Exception as e:
                    batch_responses.append(
                        {"query": query, "error": str(e), "success": False}
                    )

            batch_results.extend(batch_responses)

        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()

        # C·∫≠p nh·∫≠t session stats
        chat_sessions[session_id]["message_count"] += len(queries)

        # T√≠nh to√°n statistics
        successful_queries = [r for r in batch_results if r.get("success", False)]
        total_tokens = sum(r.get("tokens_used", 0) for r in successful_queries)
        total_function_calls = sum(
            r.get("function_calls", 0) for r in successful_queries
        )

        return jsonify(
            {
                "success": True,
                "batch_results": batch_results,
                "statistics": {
                    "total_queries": len(queries),
                    "successful_queries": len(successful_queries),
                    "failed_queries": len(queries) - len(successful_queries),
                    "total_tokens_used": total_tokens,
                    "total_function_calls": total_function_calls,
                    "processing_time_seconds": processing_time,
                    "average_time_per_query": (
                        processing_time / len(queries) if queries else 0
                    ),
                    "batch_size_used": batch_size,
                },
            }
        )

    except Exception as e:
        return (
            jsonify({"success": False, "error": f"L·ªói x·ª≠ l√Ω batch chat: {str(e)}"}),
            500,
        )


@app.route("/api/batch_expense_processing", methods=["POST"])
def batch_expense_processing():
    """X·ª≠ l√Ω batch expense processing."""
    data = request.get_json()
    expenses = data.get(
        "expenses", MOCK_EXPENSE_REPORTS
    )  # S·ª≠ d·ª•ng mock data n·∫øu kh√¥ng c√≥ input

    try:
        start_time = datetime.now()

        # S·ª≠ d·ª•ng function t·ª´ cli module
        batch_result = run_expense_batch_processing()

        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()

        # Th√™m th√¥ng tin timing
        batch_result["processing_time_seconds"] = processing_time
        batch_result["expenses_processed"] = len(expenses)

        return jsonify({"success": True, "batch_result": batch_result})

    except Exception as e:
        return (
            jsonify({"success": False, "error": f"L·ªói x·ª≠ l√Ω batch expense: {str(e)}"}),
            500,
        )


@app.route("/api/sample_questions")
def sample_questions():
    """L·∫•y danh s√°ch c√¢u h·ªèi m·∫´u."""
    return jsonify({"success": True, "questions": SAMPLE_USER_QUERIES})


@app.route("/api/batching_stats", methods=["GET"])
def get_batching_stats():
    """L·∫•y th·ªëng k√™ batching system."""
    try:
        stats = batching_queue.get_stats()
        queue_size = batching_queue.pending_requests.qsize()

        return jsonify(
            {
                "success": True,
                "stats": {
                    "enabled": ENABLE_AUTO_BATCHING,
                    "queue_size": queue_size,
                    "total_requests": stats["total_requests"],
                    "total_batches": stats["total_batches"],
                    "average_batch_size": round(stats["average_batch_size"], 2),
                    "total_tokens_saved": stats["total_tokens_saved"],
                    "config": BATCHING_CONFIG,
                    "is_running": batching_queue.is_running,
                },
            }
        )

    except Exception as e:
        return (
            jsonify(
                {"success": False, "error": f"L·ªói l·∫•y th·ªëng k√™ batching: {str(e)}"}
            ),
            500,
        )


@app.route("/api/batching_control", methods=["POST"])
def control_batching():
    """B·∫≠t/t·∫Øt batching system."""
    try:
        data = request.get_json()
        action = data.get("action", "").lower()

        global ENABLE_AUTO_BATCHING

        if action == "enable":
            ENABLE_AUTO_BATCHING = True
            if not batching_queue.is_running:
                batching_queue.start()
            message = "Auto-batching ƒë√£ ƒë∆∞·ª£c b·∫≠t"

        elif action == "disable":
            ENABLE_AUTO_BATCHING = False
            message = "Auto-batching ƒë√£ ƒë∆∞·ª£c t·∫Øt (queue v·∫´n ch·∫°y)"

        elif action == "restart":
            batching_queue.stop()
            time.sleep(0.5)
            batching_queue.start()
            message = "Batching queue ƒë√£ ƒë∆∞·ª£c restart"

        else:
            return (
                jsonify(
                    {
                        "success": False,
                        "error": "Action kh√¥ng h·ª£p l·ªá. S·ª≠ d·ª•ng: enable, disable, restart",
                    }
                ),
                400,
            )

        return jsonify(
            {
                "success": True,
                "message": message,
                "enabled": ENABLE_AUTO_BATCHING,
                "running": batching_queue.is_running,
            }
        )

    except Exception as e:
        return (
            jsonify({"success": False, "error": f"L·ªói ƒëi·ªÅu khi·ªÉn batching: {str(e)}"}),
            500,
        )


@app.route("/api/system_info")
def system_info():
    """L·∫•y th√¥ng tin h·ªá th·ªëng."""
    return jsonify(
        {
            "success": True,
            "info": {
                "total_policies": len(EXPENSE_POLICIES),
                "sample_expenses": len(MOCK_EXPENSE_REPORTS),
                "active_sessions": len(chat_sessions),
                "openai_connected": assistant is not None,
                "base_url": os.getenv("OPENAI_BASE_URL", "Ch∆∞a thi·∫øt l·∫≠p"),
                "model": os.getenv("OPENAI_DEPLOYMENT", "GPT-4o-mini"),
                "batching_enabled": True,
                "features": {
                    "batch_chat": True,
                    "batch_expense_processing": True,
                    "parallel_processing": True,
                    "token_optimization": True,
                },
            },
        }
    )


@app.route("/api/clear_session", methods=["POST"])
def clear_session():
    """X√≥a phi√™n chat."""
    data = request.get_json()
    session_id = data.get("session_id")

    if session_id and session_id in chat_sessions:
        chat_sessions[session_id]["assistant"].clear_conversation()
        chat_sessions[session_id]["message_count"] = 0

        return jsonify({"success": True, "message": "Phi√™n chat ƒë√£ ƒë∆∞·ª£c x√≥a"})

    return jsonify({"success": False, "error": "Phi√™n chat kh√¥ng t·ªìn t·∫°i"}), 400


@app.route("/api/session_stats/<session_id>")
def session_stats(session_id):
    """L·∫•y th·ªëng k√™ phi√™n chat."""
    if session_id in chat_sessions:
        session_data = chat_sessions[session_id]
        return jsonify(
            {
                "success": True,
                "stats": {
                    "created_at": session_data["created_at"],
                    "message_count": session_data["message_count"],
                    "conversation_summary": session_data[
                        "assistant"
                    ].get_conversation_summary(),
                },
            }
        )

    return jsonify({"success": False, "error": "Phi√™n chat kh√¥ng t·ªìn t·∫°i"}), 404


@app.errorhandler(404)
def not_found(error):
    """X·ª≠ l√Ω l·ªói 404."""
    return jsonify({"success": False, "error": "Kh√¥ng t√¨m th·∫•y trang"}), 404


@app.errorhandler(500)
def internal_error(error):
    """X·ª≠ l√Ω l·ªói 500."""
    return jsonify({"success": False, "error": "L·ªói m√°y ch·ªß n·ªôi b·ªô"}), 500


if __name__ == "__main__":
    # T·∫°o th∆∞ m·ª•c templates n·∫øu ch∆∞a c√≥
    os.makedirs("templates", exist_ok=True)
    os.makedirs("static/css", exist_ok=True)
    os.makedirs("static/js", exist_ok=True)

    print("üåê Kh·ªüi ƒë·ªông ·ª©ng d·ª•ng web Tr·ª£ L√Ω B√°o C√°o Chi Ph√≠...")
    print("üìä Tr·∫°ng th√°i h·ªá th·ªëng:")
    print(f"   ‚Ä¢ Assistant: {'‚úÖ S·∫µn s√†ng' if assistant else '‚ùå L·ªói'}")
    print(f"   ‚Ä¢ Ch√≠nh s√°ch: {len(EXPENSE_POLICIES)} danh m·ª•c")
    print(f"   ‚Ä¢ D·ªØ li·ªáu m·∫´u: {len(MOCK_EXPENSE_REPORTS)} chi ph√≠")
    print(f"   üÜï Batching: ‚úÖ ƒê√£ t√≠ch h·ª£p")
    print(f"   üÜï T√≠nh nƒÉng:")
    print(f"      ‚Ä¢ Batch Chat Processing")
    print(f"      ‚Ä¢ Batch Expense Processing")
    print(f"      ‚Ä¢ Token Usage Optimization")
    print("üöÄ M·ªü tr√¨nh duy·ªát v√† truy c·∫≠p: http://localhost:5000")
    print("üìã API Endpoints:")
    print("   ‚Ä¢ POST /api/batch_chat - X·ª≠ l√Ω nhi·ªÅu chat queries")
    print("   ‚Ä¢ POST /api/batch_expense_processing - Batch expense processing")
    print("üÜï Auto-Batching Endpoints:")
    print("   ‚Ä¢ GET /api/batching_stats - Th·ªëng k√™ batching")
    print("   ‚Ä¢ POST /api/batching_control - ƒêi·ªÅu khi·ªÉn batching")

    # üöÄ Kh·ªüi ƒë·ªông Auto-Batching System
    if ENABLE_AUTO_BATCHING:
        batching_queue.start()
        print("‚úÖ Auto-batching system started")

    try:
        app.run(debug=True, host="0.0.0.0", port=5000)
    finally:
        # Cleanup khi server d·ª´ng
        batching_queue.stop()
