"""
üß† INTELLIGENT CONVERSATION SUMMARIZATION SYSTEM
===================================================

Gi·∫£i ph√°p t·ªëi ∆∞u cho vi·ªác l∆∞u tr·ªØ v√† t√≥m t·∫Øt l·ªãch s·ª≠ chat:
- Sliding Window Memory v·ªõi auto-summarization
- Context-aware Summarization cho expense domain  
- Token-efficient Storage trong ChromaDB
- Semantic Compression ƒë·ªÉ gi·ªØ th√¥ng tin quan tr·ªçng

Author: AI Expert Team
Date: August 2025
"""

import json
import re
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from openai import OpenAI
import tiktoken
import chromadb
from chromadb.utils import embedding_functions


@dataclass
class ConversationSegment:
    """ƒê·∫°i di·ªán cho m·ªôt ƒëo·∫°n h·ªôi tho·∫°i ƒë√£ ƒë∆∞·ª£c t√≥m t·∫Øt"""
    start_time: str
    end_time: str
    message_count: int
    summary: str
    key_expenses: List[Dict[str, Any]]
    important_context: Dict[str, Any]
    tokens_saved: int
    original_tokens: int


class IntelligentConversationSummarizer:
    """
    üéØ H·ªá th·ªëng t√≥m t·∫Øt h·ªôi tho·∫°i th√¥ng minh cho expense domain
    
    Features:
    - Sliding window v·ªõi threshold t·ª± ƒë·ªông
    - Context-aware summarization
    - Expense-specific information extraction
    - Token optimization v·ªõi semantic preservation
    """
    
    def __init__(self, openai_client: OpenAI, max_window_size: int = 10, 
                 summarize_threshold: int = 8, max_tokens_per_summary: int = 200):
        """
        Kh·ªüi t·∫°o conversation summarizer
        
        Args:
            openai_client: OpenAI client instance
            max_window_size: S·ªë messages t·ªëi ƒëa trong active window
            summarize_threshold: Trigger summarization khi ƒë·∫°t threshold
            max_tokens_per_summary: Gi·ªõi h·∫°n tokens cho m·ªói summary
        """
        self.client = openai_client
        self.max_window_size = max_window_size
        self.summarize_threshold = summarize_threshold
        self.max_tokens_per_summary = max_tokens_per_summary
        
        # Token counter ƒë·ªÉ t·ªëi ∆∞u cost
        self.encoding = tiktoken.encoding_for_model("gpt-4")
        
        # ChromaDB cho l∆∞u tr·ªØ summaries
        self.chroma_client = chromadb.PersistentClient(path="./data/conversation_summaries")
        self.embedding_fn = embedding_functions.DefaultEmbeddingFunction()
        
        # Collection cho conversation summaries
        self.summaries_collection = self.chroma_client.get_or_create_collection(
            name="conversation_summaries",
            embedding_function=self.embedding_fn,
            metadata={"description": "Summarized conversation segments"}
        )
        
        # Collection cho active conversations
        self.active_conversations = {}
        
        # Expense domain keywords for context extraction
        self.expense_keywords = {
            'amounts': r'(\d+(?:[,\.]\d+)?)\s*(?:tri·ªáu|tr|ngh√¨n|k|VND|vnƒë|ƒë·ªìng)',
            'categories': ['ƒÉn', 'u·ªëng', 'meal', 'kh√°ch s·∫°n', 'hotel', 'ƒëi l·∫°i', 'travel', 'vƒÉn ph√≤ng', 'office'],
            'actions': ['k√™ khai', 'declare', 'b√°o c√°o', 'report', 'ho√†n tr·∫£', 'reimburse'],
            'policies': ['ch√≠nh s√°ch', 'policy', 'quy ƒë·ªãnh', 'gi·ªõi h·∫°n', 'limit', 'h√≥a ƒë∆°n', 'receipt']
        }
    
    def count_tokens(self, text: str) -> int:
        """ƒê·∫øm s·ªë tokens trong text"""
        return len(self.encoding.encode(text))
    
    def extract_expense_context(self, messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Tr√≠ch xu·∫•t context quan tr·ªçng li√™n quan ƒë·∫øn expense t·ª´ messages
        
        Returns:
            Dict ch·ª©a expenses, policies, user intents ƒë√£ ƒë∆∞·ª£c extract
        """
        context = {
            'declared_expenses': [],
            'policy_questions': [],
            'calculation_requests': [],
            'report_requests': [],
            'user_preferences': {},
            'important_numbers': []
        }
        
        for msg in messages:
            content = msg.get('content', '').lower()
            role = msg.get('role', '')
            
            # Extract declared expenses
            amounts = re.findall(self.expense_keywords['amounts'], content)
            if amounts and role == 'user':
                expense_info = {
                    'amounts': amounts,
                    'content': msg.get('content', '')[:100],
                    'timestamp': msg.get('timestamp', '')
                }
                context['declared_expenses'].append(expense_info)
            
            # Extract policy questions
            if any(keyword in content for keyword in self.expense_keywords['policies']):
                context['policy_questions'].append({
                    'question': msg.get('content', '')[:100],
                    'timestamp': msg.get('timestamp', '')
                })
            
            # Extract calculation/report requests
            if any(action in content for action in self.expense_keywords['actions']):
                if 'b√°o c√°o' in content or 'report' in content:
                    context['report_requests'].append(msg.get('content', '')[:100])
                else:
                    context['calculation_requests'].append(msg.get('content', '')[:100])
        
        return context
    
    def create_domain_specific_summary(self, messages: List[Dict[str, Any]]) -> str:
        """
        T·∫°o summary t·ªëi ∆∞u cho expense domain v·ªõi focus v√†o th√¥ng tin quan tr·ªçng
        """
        expense_context = self.extract_expense_context(messages)
        
        # T·∫°o prompt t·ªëi ∆∞u cho expense summarization
        system_prompt = """B·∫°n l√† chuy√™n gia t√≥m t·∫Øt h·ªôi tho·∫°i cho h·ªá th·ªëng b√°o c√°o chi ph√≠. 
        
        NHI·ªÜM V·ª§: T√≥m t·∫Øt h·ªôi tho·∫°i sau ƒë√¢y, t·∫≠p trung v√†o:
        1. üí∞ Chi ph√≠ ƒë√£ k√™ khai (s·ªë ti·ªÅn, danh m·ª•c, m√¥ t·∫£)
        2. ‚ùì C√¢u h·ªèi v·ªÅ ch√≠nh s√°ch c√¥ng ty
        3. üìä Y√™u c·∫ßu t√≠nh to√°n ho·∫∑c b√°o c√°o
        4. üéØ Context quan tr·ªçng cho cu·ªôc tr√≤ chuy·ªán ti·∫øp theo
        
        ƒê·ªäNH D·∫†NG OUTPUT:
        üí∞ CHI PH√ç: [li·ªát k√™ chi ph√≠ ƒë√£ k√™ khai]
        ‚ùì CH√çNH S√ÅCH: [c√¢u h·ªèi v·ªÅ policies ƒë√£ ƒë∆∞·ª£c tr·∫£ l·ªùi]
        üìä Y√äU C·∫¶U: [c√°c y√™u c·∫ßu t√≠nh to√°n/b√°o c√°o]
        üîî CONTEXT: [th√¥ng tin quan tr·ªçng c·∫ßn nh·ªõ]
        
        Gi·ªØ summary ng·∫Øn g·ªçn nh∆∞ng ƒë·∫ßy ƒë·ªß th√¥ng tin quan tr·ªçng (max 300 words)."""
        
        # Chu·∫©n b·ªã conversation text
        conversation_text = "\n".join([
            f"{msg['role'].upper()}: {msg['content']}" 
            for msg in messages[-self.summarize_threshold:]
        ])
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"H·ªôi tho·∫°i c·∫ßn t√≥m t·∫Øt:\n\n{conversation_text}"}
                ],
                max_tokens=self.max_tokens_per_summary,
                temperature=0.3
            )
            
            summary = response.choices[0].message.content.strip()
            return summary
            
        except Exception as e:
            # Fallback summary n·∫øu API call fails
            return self.create_fallback_summary(messages, expense_context)
    
    def create_fallback_summary(self, messages: List[Dict[str, Any]], 
                               expense_context: Dict[str, Any]) -> str:
        """T·∫°o summary backup kh√¥ng c·∫ßn API call"""
        summary_parts = []
        
        if expense_context['declared_expenses']:
            expenses_text = f"üí∞ CHI PH√ç: {len(expense_context['declared_expenses'])} kho·∫£n ƒë√£ k√™ khai"
            summary_parts.append(expenses_text)
        
        if expense_context['policy_questions']:
            policies_text = f"‚ùì CH√çNH S√ÅCH: {len(expense_context['policy_questions'])} c√¢u h·ªèi"
            summary_parts.append(policies_text)
        
        if expense_context['report_requests']:
            reports_text = f"üìä Y√äU C·∫¶U: {len(expense_context['report_requests'])} y√™u c·∫ßu b√°o c√°o"
            summary_parts.append(reports_text)
        
        summary_parts.append(f"üîî CONTEXT: {len(messages)} tin nh·∫Øn ƒë√£ ƒë∆∞·ª£c t√≥m t·∫Øt")
        
        return "\n".join(summary_parts)
    
    def should_summarize(self, session_id: str) -> bool:
        """Ki·ªÉm tra xem c√≥ n√™n trigger summarization kh√¥ng"""
        if session_id not in self.active_conversations:
            return False
        
        messages = self.active_conversations[session_id]['messages']
        return len(messages) >= self.summarize_threshold
    
    def add_message(self, session_id: str, message: Dict[str, Any]) -> Dict[str, Any]:
        """
        Th√™m message v√†o conversation v√† trigger summarization n·∫øu c·∫ßn
        
        Returns:
            Dict ch·ª©a info v·ªÅ summarization status v√† token savings
        """
        # Kh·ªüi t·∫°o session n·∫øu ch∆∞a t·ªìn t·∫°i
        if session_id not in self.active_conversations:
            self.active_conversations[session_id] = {
                'messages': [],
                'summaries': [],
                'total_tokens_saved': 0,
                'created_at': datetime.now().isoformat()
            }
        
        # Th√™m timestamp v√†o message
        message['timestamp'] = datetime.now().isoformat()
        
        # Th√™m message v√†o active window
        self.active_conversations[session_id]['messages'].append(message)
        
        result = {
            'summarized': False,
            'tokens_saved': 0,
            'summary_created': None,
            'active_messages': len(self.active_conversations[session_id]['messages'])
        }
        
        # Ki·ªÉm tra xem c√≥ c·∫ßn summarize kh√¥ng
        if self.should_summarize(session_id):
            summary_result = self.summarize_conversation_window(session_id)
            result.update(summary_result)
        
        return result
    
    def summarize_conversation_window(self, session_id: str) -> Dict[str, Any]:
        """
        T√≥m t·∫Øt conversation window hi·ªán t·∫°i v√† l∆∞u v√†o ChromaDB
        """
        if session_id not in self.active_conversations:
            return {'error': 'Session not found'}
        
        messages = self.active_conversations[session_id]['messages']
        
        # T√≠nh to√°n tokens tr∆∞·ªõc khi summarize
        original_tokens = sum(self.count_tokens(msg.get('content', '')) for msg in messages)
        
        # T·∫°o summary
        summary_text = self.create_domain_specific_summary(messages)
        summary_tokens = self.count_tokens(summary_text)
        tokens_saved = original_tokens - summary_tokens
        
        # Extract key information
        expense_context = self.extract_expense_context(messages)
        
        # T·∫°o conversation segment
        segment = ConversationSegment(
            start_time=messages[0].get('timestamp', ''),
            end_time=messages[-1].get('timestamp', ''),
            message_count=len(messages),
            summary=summary_text,
            key_expenses=expense_context['declared_expenses'],
            important_context=expense_context,
            tokens_saved=tokens_saved,
            original_tokens=original_tokens
        )
        
        # L∆∞u v√†o ChromaDB
        segment_id = f"{session_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        try:
            self.summaries_collection.add(
                documents=[summary_text],
                ids=[segment_id],
                metadatas=[{
                    'session_id': session_id,
                    'start_time': segment.start_time,
                    'end_time': segment.end_time,
                    'message_count': segment.message_count,
                    'tokens_saved': tokens_saved,
                    'original_tokens': original_tokens,
                    'expense_count': len(expense_context['declared_expenses']),
                    'policy_questions': len(expense_context['policy_questions']),
                    'summary_type': 'conversation_segment'
                }]
            )
            
            # L∆∞u v√†o active conversation
            self.active_conversations[session_id]['summaries'].append(segment)
            self.active_conversations[session_id]['total_tokens_saved'] += tokens_saved
            
            # Gi·ªØ l·∫°i ch·ªâ m·ªôt s·ªë messages g·∫ßn nh·∫•t
            keep_recent = max(2, self.max_window_size - self.summarize_threshold)
            self.active_conversations[session_id]['messages'] = messages[-keep_recent:]
            
            return {
                'summarized': True,
                'tokens_saved': tokens_saved,
                'summary_created': summary_text,
                'segment_id': segment_id,
                'active_messages': len(self.active_conversations[session_id]['messages'])
            }
            
        except Exception as e:
            return {
                'summarized': False,
                'error': f"Failed to save summary: {str(e)}",
                'tokens_saved': 0
            }
    
    def get_conversation_context(self, session_id: str, max_summaries: int = 3) -> str:
        """
        L·∫•y context t·ª´ summaries v√† active messages ƒë·ªÉ g·ª≠i c√πng request m·ªõi
        
        Returns:
            Context string ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a ƒë·ªÉ g·ª≠i k√®m v·ªõi current conversation
        """
        if session_id not in self.active_conversations:
            return ""
        
        context_parts = []
        
        # L·∫•y recent summaries t·ª´ ChromaDB
        try:
            recent_summaries = self.summaries_collection.query(
                query_texts=[""],
                n_results=max_summaries,
                where={"session_id": session_id}
            )
            
            if recent_summaries['documents']:
                context_parts.append("üìö L·ªäCH S·ª¨ ƒê√É T√ìM T·∫ÆT:")
                for summary in recent_summaries['documents'][0]:
                    context_parts.append(f"‚Ä¢ {summary}")
                context_parts.append("")
        except Exception:
            pass
        
        # Th√™m active messages
        active_messages = self.active_conversations[session_id]['messages']
        if active_messages:
            context_parts.append("üí¨ TIN NH·∫ÆN G√ÇN ƒê√ÇY:")
            for msg in active_messages[-5:]:  # Ch·ªâ l·∫•y 5 messages g·∫ßn nh·∫•t
                role_emoji = "üë§" if msg['role'] == 'user' else "ü§ñ"
                context_parts.append(f"{role_emoji} {msg['content'][:100]}...")
        
        return "\n".join(context_parts)
    
    def get_session_stats(self, session_id: str) -> Dict[str, Any]:
        """L·∫•y th·ªëng k√™ v·ªÅ session"""
        if session_id not in self.active_conversations:
            return {'error': 'Session not found'}
        
        session_data = self.active_conversations[session_id]
        
        return {
            'session_id': session_id,
            'active_messages': len(session_data['messages']),
            'total_summaries': len(session_data['summaries']),
            'total_tokens_saved': session_data['total_tokens_saved'],
            'created_at': session_data['created_at'],
            'last_activity': session_data['messages'][-1]['timestamp'] if session_data['messages'] else None
        }
    
    def cleanup_old_sessions(self, days_threshold: int = 7):
        """D·ªçn d·∫πp c√°c sessions c≈©"""
        cutoff_date = datetime.now() - timedelta(days=days_threshold)
        
        sessions_to_remove = []
        for session_id, data in self.active_conversations.items():
            created_at = datetime.fromisoformat(data['created_at'])
            if created_at < cutoff_date:
                sessions_to_remove.append(session_id)
        
        for session_id in sessions_to_remove:
            del self.active_conversations[session_id]
        
        return len(sessions_to_remove)


# Utility functions ƒë·ªÉ integrate v·ªõi existing system
def create_summarizer(openai_client: OpenAI) -> IntelligentConversationSummarizer:
    """Factory function ƒë·ªÉ t·∫°o summarizer instance"""
    return IntelligentConversationSummarizer(
        openai_client=openai_client,
        max_window_size=10,
        summarize_threshold=8,
        max_tokens_per_summary=200
    )


def integrate_with_expense_assistant(assistant_instance, summarizer: IntelligentConversationSummarizer):
    """
    Integrate summarizer v·ªõi ExpenseAssistant existing instance
    
    Patches existing methods ƒë·ªÉ th√™m auto-summarization
    """
    original_add_user_message = assistant_instance.add_user_message
    original_add_assistant_message = assistant_instance.add_assistant_message
    
    # Generate session ID cho assistant instance
    if not hasattr(assistant_instance, '_summarizer_session_id'):
        assistant_instance._summarizer_session_id = f"assistant_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    def enhanced_add_user_message(content):
        # G·ªçi original method
        original_add_user_message(content)
        
        # Th√™m v√†o summarizer
        summarizer.add_message(
            assistant_instance._summarizer_session_id,
            {'role': 'user', 'content': content}
        )
    
    def enhanced_add_assistant_message(content):
        # G·ªçi original method
        original_add_assistant_message(content)
        
        # Th√™m v√†o summarizer
        summarizer.add_message(
            assistant_instance._summarizer_session_id,
            {'role': 'assistant', 'content': content}
        )
    
    # Patch methods
    assistant_instance.add_user_message = enhanced_add_user_message
    assistant_instance.add_assistant_message = enhanced_add_assistant_message
    assistant_instance._summarizer = summarizer
    
    return assistant_instance


if __name__ == "__main__":
    # Demo usage
    from openai import OpenAI
    import os
    from dotenv import load_dotenv
    
    load_dotenv()
    
    client = OpenAI(
        api_key=os.getenv("AZURE_OPENAI_LLM_API_KEY"),
        base_url=os.getenv("AZURE_OPENAI_LLM_ENDPOINT")
    )
    
    # T·∫°o summarizer
    summarizer = create_summarizer(client)
    
    # Demo conversation
    session_id = "demo_session"
    
    # Simulate conversation
    messages = [
        {"role": "user", "content": "T√¥i mu·ªën k√™ khai chi ph√≠ ƒÉn tr∆∞a 150k"},
        {"role": "assistant", "content": "ƒê√£ ghi nh·∫≠n chi ph√≠ ƒÉn tr∆∞a 150,000 VND"},
        {"role": "user", "content": "C√≤n chi ph√≠ taxi 50k"},
        {"role": "assistant", "content": "ƒê√£ ghi nh·∫≠n chi ph√≠ taxi 50,000 VND"},
        {"role": "user", "content": "Gi·ªõi h·∫°n chi ph√≠ ƒÉn u·ªëng l√† bao nhi·ªÅu?"},
        {"role": "assistant", "content": "Gi·ªõi h·∫°n chi ph√≠ ƒÉn u·ªëng l√† 1,000,000 VND/ng√†y"},
        {"role": "user", "content": "T√¥i c·∫ßn b√°o c√°o chi ph√≠ th√°ng n√†y"},
        {"role": "assistant", "content": "ƒêang t·∫°o b√°o c√°o chi ph√≠ cho b·∫°n..."},
        {"role": "user", "content": "Chi ph√≠ kh√°ch s·∫°n 2 tri·ªáu c√≥ ƒë∆∞·ª£c ho√†n kh√¥ng?"},
        {"role": "assistant", "content": "Chi ph√≠ kh√°ch s·∫°n 2,000,000 VND ƒë∆∞·ª£c ho√†n tr·∫£ n·∫øu c√≥ h√≥a ƒë∆°n"},
    ]
    
    # Add messages v√† trigger summarization
    for msg in messages:
        result = summarizer.add_message(session_id, msg)
        if result['summarized']:
            print(f"‚úÖ Summarized! Tokens saved: {result['tokens_saved']}")
            print(f"üìù Summary: {result['summary_created'][:100]}...")
    
    # L·∫•y context ƒë·ªÉ s·ª≠ d·ª•ng
    context = summarizer.get_conversation_context(session_id)
    print(f"\nüîÑ Context for next conversation:\n{context}")
    
    # Session stats
    stats = summarizer.get_session_stats(session_id)
    print(f"\nüìä Session Stats: {stats}")
