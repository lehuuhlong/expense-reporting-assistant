"""
ğŸ§  INTELLIGENT CONVERSATION SUMMARIZATION SYSTEM
===================================================

Giáº£i phÃ¡p tá»‘i Æ°u cho viá»‡c lÆ°u trá»¯ vÃ  tÃ³m táº¯t lá»‹ch sá»­ chat:
- Sliding Window Memory vá»›i auto-summarization
- Context-aware Summarization cho expense domain  
- Token-efficient Storage trong ChromaDB
- Semantic Compression Ä‘á»ƒ giá»¯ thÃ´ng tin quan trá»ng

Author: AI Expert Team
Date: August 2025
"""

import json
import re
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from openai import OpenAI
import tiktoken
import chromadb
from chromadb.utils import embedding_functions


@dataclass
class ConversationSegment:
    """Äáº¡i diá»‡n cho má»™t Ä‘oáº¡n há»™i thoáº¡i Ä‘Ã£ Ä‘Æ°á»£c tÃ³m táº¯t"""
    start_time: str
    end_time: str
    message_count: int
    summary: str
    key_expenses: List[Dict[str, Any]]
    important_context: Dict[str, Any]
    tokens_saved: int
    original_tokens: int


class IntelligentConversationSummarizer:
    """
    ğŸ¯ Há»‡ thá»‘ng tÃ³m táº¯t há»™i thoáº¡i thÃ´ng minh cho expense domain
    
    Features:
    - Sliding window vá»›i threshold tá»± Ä‘á»™ng
    - Context-aware summarization
    - Expense-specific information extraction
    - Token optimization vá»›i semantic preservation
    """
    
    def __init__(self, openai_client: OpenAI, max_window_size: int = 10, 
                 summarize_threshold: int = 8, max_tokens_per_summary: int = 200):
        """
        Khá»Ÿi táº¡o conversation summarizer
        
        Args:
            openai_client: OpenAI client instance
            max_window_size: Sá»‘ messages tá»‘i Ä‘a trong active window
            summarize_threshold: Trigger summarization khi Ä‘áº¡t threshold
            max_tokens_per_summary: Giá»›i háº¡n tokens cho má»—i summary
        """
        self.client = openai_client
        self.max_window_size = max_window_size
        self.summarize_threshold = summarize_threshold
        self.max_tokens_per_summary = max_tokens_per_summary
        
        # Token counter Ä‘á»ƒ tá»‘i Æ°u cost
        self.encoding = tiktoken.encoding_for_model("gpt-4")
        
        # ChromaDB cho lÆ°u trá»¯ summaries
        self.chroma_client = chromadb.PersistentClient(path="./data/conversation_summaries")
        self.embedding_fn = embedding_functions.DefaultEmbeddingFunction()
        
        # Collection cho conversation summaries
        self.summaries_collection = self.chroma_client.get_or_create_collection(
            name="conversation_summaries",
            embedding_function=self.embedding_fn,
            metadata={"description": "Summarized conversation segments"}
        )
        
        # Collection cho active conversations
        self.active_conversations = {}
        
        # Expense domain keywords for context extraction
        self.expense_keywords = {
            'amounts': r'(\d+(?:[,\.]\d+)?)\s*(?:triá»‡u|tr|nghÃ¬n|k|VND|vnÄ‘|Ä‘á»“ng)',
            'categories': ['Äƒn', 'uá»‘ng', 'meal', 'khÃ¡ch sáº¡n', 'hotel', 'Ä‘i láº¡i', 'travel', 'vÄƒn phÃ²ng', 'office'],
            'actions': ['kÃª khai', 'declare', 'bÃ¡o cÃ¡o', 'report', 'hoÃ n tráº£', 'reimburse'],
            'policies': ['chÃ­nh sÃ¡ch', 'policy', 'quy Ä‘á»‹nh', 'giá»›i háº¡n', 'limit', 'hÃ³a Ä‘Æ¡n', 'receipt']
        }
    
    def count_tokens(self, text: str) -> int:
        """Äáº¿m sá»‘ tokens trong text"""
        return len(self.encoding.encode(text))
    
    def extract_expense_context(self, messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        TrÃ­ch xuáº¥t context quan trá»ng liÃªn quan Ä‘áº¿n expense tá»« messages
        
        Returns:
            Dict chá»©a expenses, policies, user intents Ä‘Ã£ Ä‘Æ°á»£c extract
        """
        context = {
            'declared_expenses': [],
            'policy_questions': [],
            'calculation_requests': [],
            'report_requests': [],
            'user_preferences': {},
            'important_numbers': []
        }
        
        for msg in messages:
            content = msg.get('content', '').lower()
            role = msg.get('role', '')
            
            # Extract declared expenses
            amounts = re.findall(self.expense_keywords['amounts'], content)
            if amounts and role == 'user':
                expense_info = {
                    'amounts': amounts,
                    'content': msg.get('content', '')[:100],
                    'timestamp': msg.get('timestamp', '')
                }
                context['declared_expenses'].append(expense_info)
            
            # Extract policy questions
            if any(keyword in content for keyword in self.expense_keywords['policies']):
                context['policy_questions'].append({
                    'question': msg.get('content', '')[:100],
                    'timestamp': msg.get('timestamp', '')
                })
            
            # Extract calculation/report requests
            if any(action in content for action in self.expense_keywords['actions']):
                if 'bÃ¡o cÃ¡o' in content or 'report' in content:
                    context['report_requests'].append(msg.get('content', '')[:100])
                else:
                    context['calculation_requests'].append(msg.get('content', '')[:100])
        
        return context
    
    def create_domain_specific_summary(self, messages: List[Dict[str, Any]]) -> str:
        """
        Táº¡o summary tá»‘i Æ°u cho expense domain vá»›i focus vÃ o thÃ´ng tin quan trá»ng
        """
        expense_context = self.extract_expense_context(messages)
        
        # Táº¡o prompt tá»‘i Æ°u cho expense summarization
        system_prompt = """Báº¡n lÃ  chuyÃªn gia tÃ³m táº¯t há»™i thoáº¡i cho há»‡ thá»‘ng bÃ¡o cÃ¡o chi phÃ­. 
        
        NHIá»†M Vá»¤: TÃ³m táº¯t há»™i thoáº¡i sau Ä‘Ã¢y, táº­p trung vÃ o:
        1. ğŸ’° Chi phÃ­ Ä‘Ã£ kÃª khai (sá»‘ tiá»n, danh má»¥c, mÃ´ táº£)
        2. â“ CÃ¢u há»i vá» chÃ­nh sÃ¡ch cÃ´ng ty
        3. ğŸ“Š YÃªu cáº§u tÃ­nh toÃ¡n hoáº·c bÃ¡o cÃ¡o
        4. ğŸ¯ Context quan trá»ng cho cuá»™c trÃ² chuyá»‡n tiáº¿p theo
        
        Äá»ŠNH Dáº NG OUTPUT:
        ğŸ’° CHI PHÃ: [liá»‡t kÃª chi phÃ­ Ä‘Ã£ kÃª khai]
        â“ CHÃNH SÃCH: [cÃ¢u há»i vá» policies Ä‘Ã£ Ä‘Æ°á»£c tráº£ lá»i]
        ğŸ“Š YÃŠU Cáº¦U: [cÃ¡c yÃªu cáº§u tÃ­nh toÃ¡n/bÃ¡o cÃ¡o]
        ğŸ”” CONTEXT: [thÃ´ng tin quan trá»ng cáº§n nhá»›]
        
        Giá»¯ summary ngáº¯n gá»n nhÆ°ng Ä‘áº§y Ä‘á»§ thÃ´ng tin quan trá»ng (max 300 words)."""
        
        # Chuáº©n bá»‹ conversation text
        conversation_text = "\n".join([
            f"{msg['role'].upper()}: {msg['content']}" 
            for msg in messages[-self.summarize_threshold:]
        ])
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"Há»™i thoáº¡i cáº§n tÃ³m táº¯t:\n\n{conversation_text}"}
                ],
                max_tokens=self.max_tokens_per_summary,
                temperature=0.3
            )
            
            summary = response.choices[0].message.content.strip()
            return summary
            
        except Exception as e:
            # Fallback summary náº¿u API call fails
            return self.create_fallback_summary(messages, expense_context)
    
    def create_fallback_summary(self, messages: List[Dict[str, Any]], 
                               expense_context: Dict[str, Any]) -> str:
        """Táº¡o summary backup khÃ´ng cáº§n API call"""
        summary_parts = []
        
        if expense_context['declared_expenses']:
            expenses_text = f"ğŸ’° CHI PHÃ: {len(expense_context['declared_expenses'])} khoáº£n Ä‘Ã£ kÃª khai"
            summary_parts.append(expenses_text)
        
        if expense_context['policy_questions']:
            policies_text = f"â“ CHÃNH SÃCH: {len(expense_context['policy_questions'])} cÃ¢u há»i"
            summary_parts.append(policies_text)
        
        if expense_context['report_requests']:
            reports_text = f"ğŸ“Š YÃŠU Cáº¦U: {len(expense_context['report_requests'])} yÃªu cáº§u bÃ¡o cÃ¡o"
            summary_parts.append(reports_text)
        
        summary_parts.append(f"ğŸ”” CONTEXT: {len(messages)} tin nháº¯n Ä‘Ã£ Ä‘Æ°á»£c tÃ³m táº¯t")
        
        return "\n".join(summary_parts)
    
    def should_summarize(self, session_id: str) -> bool:
        """Kiá»ƒm tra xem cÃ³ nÃªn trigger summarization khÃ´ng"""
        if session_id not in self.active_conversations:
            return False
        
        messages = self.active_conversations[session_id]['messages']
        return len(messages) >= self.summarize_threshold
    
    def add_message(self, session_id: str, message: Dict[str, Any]) -> Dict[str, Any]:
        """
        ThÃªm message vÃ o conversation vÃ  trigger summarization náº¿u cáº§n
        
        Returns:
            Dict chá»©a info vá» summarization status vÃ  token savings
        """
        # Khá»Ÿi táº¡o session náº¿u chÆ°a tá»“n táº¡i
        if session_id not in self.active_conversations:
            self.active_conversations[session_id] = {
                'messages': [],
                'summaries': [],
                'total_tokens_saved': 0,
                'created_at': datetime.now().isoformat()
            }
        
        # ThÃªm timestamp vÃ o message
        message['timestamp'] = datetime.now().isoformat()
        
        # ThÃªm message vÃ o active window
        self.active_conversations[session_id]['messages'].append(message)
        
        result = {
            'summarized': False,
            'tokens_saved': 0,
            'summary_created': None,
            'active_messages': len(self.active_conversations[session_id]['messages'])
        }
        
        # Kiá»ƒm tra xem cÃ³ cáº§n summarize khÃ´ng
        if self.should_summarize(session_id):
            summary_result = self.summarize_conversation_window(session_id)
            result.update(summary_result)
        
        return result
    
    def summarize_conversation_window(self, session_id: str) -> Dict[str, Any]:
        """
        TÃ³m táº¯t conversation window hiá»‡n táº¡i vÃ  lÆ°u vÃ o ChromaDB
        """
        if session_id not in self.active_conversations:
            return {'error': 'Session not found'}
        
        messages = self.active_conversations[session_id]['messages']
        
        # TÃ­nh toÃ¡n tokens trÆ°á»›c khi summarize
        original_tokens = sum(self.count_tokens(msg.get('content', '')) for msg in messages)
        
        # Táº¡o summary
        summary_text = self.create_domain_specific_summary(messages)
        summary_tokens = self.count_tokens(summary_text)
        tokens_saved = original_tokens - summary_tokens
        
        # Extract key information
        expense_context = self.extract_expense_context(messages)
        
        # Táº¡o conversation segment
        segment = ConversationSegment(
            start_time=messages[0].get('timestamp', ''),
            end_time=messages[-1].get('timestamp', ''),
            message_count=len(messages),
            summary=summary_text,
            key_expenses=expense_context['declared_expenses'],
            important_context=expense_context,
            tokens_saved=tokens_saved,
            original_tokens=original_tokens
        )
        
        # LÆ°u vÃ o ChromaDB
        segment_id = f"{session_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        try:
            self.summaries_collection.add(
                documents=[summary_text],
                ids=[segment_id],
                metadatas=[{
                    'session_id': session_id,
                    'start_time': segment.start_time,
                    'end_time': segment.end_time,
                    'message_count': segment.message_count,
                    'tokens_saved': tokens_saved,
                    'original_tokens': original_tokens,
                    'expense_count': len(expense_context['declared_expenses']),
                    'policy_questions': len(expense_context['policy_questions']),
                    'summary_type': 'conversation_segment'
                }]
            )
            
            # LÆ°u vÃ o active conversation
            self.active_conversations[session_id]['summaries'].append(segment)
            self.active_conversations[session_id]['total_tokens_saved'] += tokens_saved
            
            # Giá»¯ láº¡i chá»‰ má»™t sá»‘ messages gáº§n nháº¥t
            keep_recent = max(2, self.max_window_size - self.summarize_threshold)
            self.active_conversations[session_id]['messages'] = messages[-keep_recent:]
            
            return {
                'summarized': True,
                'tokens_saved': tokens_saved,
                'summary_created': summary_text,
                'segment_id': segment_id,
                'active_messages': len(self.active_conversations[session_id]['messages'])
            }
            
        except Exception as e:
            return {
                'summarized': False,
                'error': f"Failed to save summary: {str(e)}",
                'tokens_saved': 0
            }
    
    def get_conversation_context(self, session_id: str, max_summaries: int = 3) -> str:
        """
        Láº¥y context tá»« summaries vÃ  active messages Ä‘á»ƒ gá»­i cÃ¹ng request má»›i
        
        Returns:
            Context string Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a Ä‘á»ƒ gá»­i kÃ¨m vá»›i current conversation
        """
        if session_id not in self.active_conversations:
            return ""
        
        context_parts = []
        
        # Láº¥y recent summaries tá»« ChromaDB
        try:
            recent_summaries = self.summaries_collection.query(
                query_texts=[""],
                n_results=max_summaries,
                where={"session_id": session_id}
            )
            
            if recent_summaries['documents']:
                context_parts.append("ğŸ“š Lá»ŠCH Sá»¬ ÄÃƒ TÃ“M Táº®T:")
                for summary in recent_summaries['documents'][0]:
                    context_parts.append(f"â€¢ {summary}")
                context_parts.append("")
        except Exception:
            pass
        
        # ThÃªm active messages
        active_messages = self.active_conversations[session_id]['messages']
        if active_messages:
            context_parts.append("ğŸ’¬ TIN NHáº®N GÃ‚N ÄÃ‚Y:")
            for msg in active_messages[-5:]:  # Chá»‰ láº¥y 5 messages gáº§n nháº¥t
                role_emoji = "ğŸ‘¤" if msg['role'] == 'user' else "ğŸ¤–"
                context_parts.append(f"{role_emoji} {msg['content'][:100]}...")
        
        return "\n".join(context_parts)
    
    def get_session_stats(self, session_id: str) -> Dict[str, Any]:
        """Láº¥y thá»‘ng kÃª vá» session"""
        if session_id not in self.active_conversations:
            return {'error': 'Session not found'}
        
        session_data = self.active_conversations[session_id]
        
        return {
            'session_id': session_id,
            'active_messages': len(session_data['messages']),
            'total_summaries': len(session_data['summaries']),
            'total_tokens_saved': session_data['total_tokens_saved'],
            'created_at': session_data['created_at'],
            'last_activity': session_data['messages'][-1]['timestamp'] if session_data['messages'] else None
        }
    
    def cleanup_old_sessions(self, days_threshold: int = 7):
        """Dá»n dáº¹p cÃ¡c sessions cÅ©"""
        cutoff_date = datetime.now() - timedelta(days=days_threshold)
        
        sessions_to_remove = []
        for session_id, data in self.active_conversations.items():
            created_at = datetime.fromisoformat(data['created_at'])
            if created_at < cutoff_date:
                sessions_to_remove.append(session_id)
        
        for session_id in sessions_to_remove:
            del self.active_conversations[session_id]
        
        return len(sessions_to_remove)


# Utility functions Ä‘á»ƒ integrate vá»›i existing system
def create_summarizer(openai_client: OpenAI) -> IntelligentConversationSummarizer:
    """Factory function Ä‘á»ƒ táº¡o summarizer instance"""
    return IntelligentConversationSummarizer(
        openai_client=openai_client,
        max_window_size=10,
        summarize_threshold=8,
        max_tokens_per_summary=200
    )


def integrate_with_expense_assistant(assistant_instance, summarizer: IntelligentConversationSummarizer):
    """
    Integrate summarizer vá»›i ExpenseAssistant existing instance
    
    Patches existing methods Ä‘á»ƒ thÃªm auto-summarization
    """
    original_add_user_message = assistant_instance.add_user_message
    original_add_assistant_message = assistant_instance.add_assistant_message
    
    # Generate session ID cho assistant instance
    if not hasattr(assistant_instance, '_summarizer_session_id'):
        assistant_instance._summarizer_session_id = f"assistant_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    def enhanced_add_user_message(content):
        # Gá»i original method
        original_add_user_message(content)
        
        # ThÃªm vÃ o summarizer
        summarizer.add_message(
            assistant_instance._summarizer_session_id,
            {'role': 'user', 'content': content}
        )
    
    def enhanced_add_assistant_message(content):
        # Gá»i original method
        original_add_assistant_message(content)
        
        # ThÃªm vÃ o summarizer
        summarizer.add_message(
            assistant_instance._summarizer_session_id,
            {'role': 'assistant', 'content': content}
        )
    
    # Patch methods
    assistant_instance.add_user_message = enhanced_add_user_message
    assistant_instance.add_assistant_message = enhanced_add_assistant_message
    assistant_instance._summarizer = summarizer
    
    return assistant_instance


if __name__ == "__main__":
    # Demo usage
    from openai import OpenAI
    import os
    from dotenv import load_dotenv
    
    load_dotenv()
    
    client = OpenAI(
        api_key=os.getenv("AZURE_OPENAI_LLM_API_KEY"),
        base_url=os.getenv("AZURE_OPENAI_LLM_ENDPOINT")
    )
    
    # Táº¡o summarizer
    summarizer = create_summarizer(client)
    
    # Demo conversation
    session_id = "demo_session"
    
    # Simulate conversation
    messages = [
        {"role": "user", "content": "TÃ´i muá»‘n kÃª khai chi phÃ­ Äƒn trÆ°a 150k"},
        {"role": "assistant", "content": "ÄÃ£ ghi nháº­n chi phÃ­ Äƒn trÆ°a 150,000 VND"},
        {"role": "user", "content": "CÃ²n chi phÃ­ taxi 50k"},
        {"role": "assistant", "content": "ÄÃ£ ghi nháº­n chi phÃ­ taxi 50,000 VND"},
        {"role": "user", "content": "Giá»›i háº¡n chi phÃ­ Äƒn uá»‘ng lÃ  bao nhiá»u?"},
        {"role": "assistant", "content": "Giá»›i háº¡n chi phÃ­ Äƒn uá»‘ng lÃ  1,000,000 VND/ngÃ y"},
        {"role": "user", "content": "TÃ´i cáº§n bÃ¡o cÃ¡o chi phÃ­ thÃ¡ng nÃ y"},
        {"role": "assistant", "content": "Äang táº¡o bÃ¡o cÃ¡o chi phÃ­ cho báº¡n..."},
        {"role": "user", "content": "Chi phÃ­ khÃ¡ch sáº¡n 2 triá»‡u cÃ³ Ä‘Æ°á»£c hoÃ n khÃ´ng?"},
        {"role": "assistant", "content": "Chi phÃ­ khÃ¡ch sáº¡n 2,000,000 VND Ä‘Æ°á»£c hoÃ n tráº£ náº¿u cÃ³ hÃ³a Ä‘Æ¡n"},
    ]
    
    # Add messages vÃ  trigger summarization
    for msg in messages:
        result = summarizer.add_message(session_id, msg)
        if result['summarized']:
            print(f"âœ… Summarized! Tokens saved: {result['tokens_saved']}")
            print(f"ğŸ“ Summary: {result['summary_created'][:100]}...")
    
    # Láº¥y context Ä‘á»ƒ sá»­ dá»¥ng
    context = summarizer.get_conversation_context(session_id)
    print(f"\nğŸ”„ Context for next conversation:\n{context}")
    
    # Session stats
    stats = summarizer.get_session_stats(session_id)
    print(f"\nğŸ“Š Session Stats: {stats}")
