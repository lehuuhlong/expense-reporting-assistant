# Tr·ª£ L√Ω B√°o C√°o Chi Ph√≠ - Module Ch√≠nh
"""
Tr·ª£ l√Ω b√°o c√°o chi ph√≠ ƒë∆∞·ª£c h·ªó tr·ª£ AI v·ªõi qu·∫£n l√Ω h·ªôi tho·∫°i,
g·ªçi h√†m v√† kh·∫£ nƒÉng h∆∞·ªõng d·∫´n ch√≠nh s√°ch.
"""

import os
import json
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import re
from dotenv import load_dotenv
from openai import OpenAI

# Load environment variables
load_dotenv()

class ExpenseAssistant:
    """
    Tr·ª£ l√Ω b√°o c√°o chi ph√≠ ƒë∆∞·ª£c h·ªó tr·ª£ AI v·ªõi qu·∫£n l√Ω h·ªôi tho·∫°i,
    g·ªçi h√†m v√† kh·∫£ nƒÉng h∆∞·ªõng d·∫´n ch√≠nh s√°ch.
    """
    
    def __init__(self, client, model="GPT-4o-mini"):
        self.client = client
        self.model = model
        self.conversation_history = []
        self.user_context = {}  # Store user-specific context
        
        # System prompt v·ªõi v√≠ d·ª• few-shot v√† ch√≠nh s√°ch c√¥ng ty
        self.system_prompt = """B·∫°n l√† Tr·ª£ L√Ω B√°o C√°o Chi Ph√≠ th√¥ng minh cho c√¥ng ty ch√∫ng t√¥i. 
Vai tr√≤ c·ªßa b·∫°n l√† gi√∫p nh√¢n vi√™n v·ªõi b√°o c√°o chi ph√≠, c√¢u h·ªèi ch√≠nh s√°ch v√† t√≠nh to√°n ho√†n ti·ªÅn.

TR√ÅCH NHI·ªÜM CH√çNH:
1. Tr·∫£ l·ªùi c√¢u h·ªèi ch√≠nh s√°ch chi ph√≠ m·ªôt c√°ch ch√≠nh x√°c
2. Gi√∫p x√°c th·ª±c v√† t√≠nh to√°n ho√†n ti·ªÅn chi ph√≠
3. H∆∞·ªõng d·∫´n ng∆∞·ªùi d√πng qua quy tr√¨nh n·ªôp chi ph√≠ ƒë√∫ng c√°ch
4. Cung c·∫•p ph·∫£n h·ªìi r√µ r√†ng, h·ªØu √≠ch v√† chuy√™n nghi·ªáp

T√ìM T·∫ÆT CH√çNH S√ÅCH CHI PH√ç C√îNG TY:
- H√≥a ƒë∆°n ƒë∆∞·ª£c y√™u c·∫ßu cho chi ph√≠ tr√™n 500.000 VNƒê
- Gi·ªõi h·∫°n ƒÉn u·ªëng: 1.000.000 VNƒê/ng√†y trong n∆∞·ªõc, 1.500.000 VNƒê/ng√†y qu·ªëc t·∫ø
- ƒêi l·∫°i c·∫ßn ph√™ duy·ªát tr∆∞·ªõc
- VƒÉn ph√≤ng ph·∫©m: gi·ªõi h·∫°n 2.000.000 VNƒê/th√°ng
- B√°o c√°o chi ph√≠ ph·∫£i n·ªôp trong v√≤ng 30 ng√†y
- T·ª∑ l·ªá xƒÉng xe: 3.000 VNƒê/km

PHONG C√ÅCH H·ªòI THO·∫†I:
- Th√¢n thi·ªán, chuy√™n nghi·ªáp v√† h·ªØu √≠ch
- ƒê·∫∑t c√¢u h·ªèi l√†m r√µ khi c·∫ßn
- Cung c·∫•p h∆∞·ªõng d·∫´n t·ª´ng b∆∞·ªõc
- S·ª≠ d·ª•ng emoji ph√π h·ª£p (üìä, üí∞, ‚úÖ, ‚ùå, ‚ö†Ô∏è)
- Lu√¥n x√°c th·ª±c th√¥ng tin so v·ªõi ch√≠nh s√°ch

G·ªåI H√ÄM:
S·ª≠ d·ª•ng c√°c h√†m c√≥ s·∫µn ƒë·ªÉ:
- T√≠nh to√°n ho√†n ti·ªÅn: calculate_reimbursement()
- X√°c th·ª±c chi ph√≠: validate_expense()
- T√¨m ki·∫øm ch√≠nh s√°ch: search_policies()
- ƒê·ªãnh d·∫°ng t√≥m t·∫Øt: format_expense_summary()

V√ç D·ª§:
Ng∆∞·ªùi d√πng: "Gi·ªõi h·∫°n chi ph√≠ ƒÉn u·ªëng l√† bao nhi·ªÅu?"
Tr·ª£ l√Ω: "üìã ƒê·ªëi v·ªõi ƒÉn u·ªëng, ch√≠nh s√°ch c√¥ng ty cho ph√©p ho√†n ti·ªÅn t·ªëi ƒëa 1.000.000 VNƒê m·ªói ng√†y cho c√¥ng t√°c trong n∆∞·ªõc v√† 1.500.000 VNƒê m·ªói ng√†y cho c√¥ng t√°c qu·ªëc t·∫ø. R∆∞·ª£u bia th∆∞·ªùng kh√¥ng ƒë∆∞·ª£c ho√†n ti·ªÅn tr·ª´ khi ti·∫øp kh√°ch h√†ng. B·∫°n c√≥ c·∫ßn gi√∫p v·ªõi chi ph√≠ ƒÉn u·ªëng c·ª• th·ªÉ n√†o kh√¥ng?"

Ng∆∞·ªùi d√πng: "T√≠nh ho√†n ti·ªÅn cho ƒÉn tr∆∞a 900.000 VNƒê, taxi 400.000 VNƒê, vƒÉn ph√≤ng ph·∫©m 2.400.000 VNƒê"
Tr·ª£ l√Ω: [S·ª≠ d·ª•ng h√†m calculate_reimbursement] "üí∞ T√¥i ƒë√£ t√≠nh to√°n ho√†n ti·ªÅn c·ªßa b·∫°n..."

Lu√¥n h·ªØu √≠ch v√† ch√≠nh x√°c v·ªõi ch√≠nh s√°ch c√¥ng ty."""

        # Kh·ªüi t·∫°o h·ªôi tho·∫°i v·ªõi system prompt
        self.conversation_history.append({
            "role": "system",
            "content": self.system_prompt
        })
    
    def add_user_message(self, message: str):
        """Th√™m tin nh·∫Øn ng∆∞·ªùi d√πng v√†o l·ªãch s·ª≠ h·ªôi tho·∫°i."""
        self.conversation_history.append({
            "role": "user",
            "content": message
        })
    
    def add_assistant_message(self, message: str):
        """Th√™m tin nh·∫Øn assistant v√†o l·ªãch s·ª≠ h·ªôi tho·∫°i."""
        self.conversation_history.append({
            "role": "assistant",
            "content": message
        })
    
    def add_function_result(self, function_name: str, result: Any):
        """Th√™m k·∫øt qu·∫£ g·ªçi h√†m v√†o l·ªãch s·ª≠ h·ªôi tho·∫°i."""
        self.conversation_history.append({
            "role": "function",
            "name": function_name,
            "content": json.dumps(result, ensure_ascii=False) if isinstance(result, dict) else str(result)
        })
    
    def get_conversation_summary(self) -> str:
        """L·∫•y t√≥m t·∫Øt cu·ªôc h·ªôi tho·∫°i hi·ªán t·∫°i."""
        user_messages = [msg for msg in self.conversation_history if msg["role"] == "user"]
        assistant_messages = [msg for msg in self.conversation_history if msg["role"] == "assistant"]
        
        return f"""üìä T√≥m T·∫Øt H·ªôi Tho·∫°i:
‚Ä¢ Tin nh·∫Øn ng∆∞·ªùi d√πng: {len(user_messages)}
‚Ä¢ Ph·∫£n h·ªìi tr·ª£ l√Ω: {len(assistant_messages)}
‚Ä¢ T·ªïng tin nh·∫Øn: {len(self.conversation_history)}
‚Ä¢ Ng·ªØ c·∫£nh ƒë∆∞·ª£c b·∫£o to√†n: ‚úÖ"""
    
    def clear_conversation(self):
        """X√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i nh∆∞ng gi·ªØ system prompt."""
        self.conversation_history = [self.conversation_history[0]]  # Gi·ªØ system prompt
        self.user_context = {}
    
    def get_response(self, user_input: str, max_retries: int = 3) -> Dict[str, Any]:
        """
        Nh·∫≠n ph·∫£n h·ªìi t·ª´ assistant v·ªõi h·ªó tr·ª£ g·ªçi h√†m.
        
        Args:
            user_input: Tin nh·∫Øn c·ªßa ng∆∞·ªùi d√πng
            max_retries: S·ªë l·∫ßn th·ª≠ l·∫°i t·ªëi ƒëa cho g·ªçi h√†m
            
        Returns:
            Dictionary v·ªõi chi ti·∫øt ph·∫£n h·ªìi
        """
    
    def process_batch_requests(self, user_inputs: List[str], batch_size: int = 5) -> List[Dict[str, Any]]:
        """
        X·ª≠ l√Ω nhi·ªÅu request c√πng l√∫c v·ªõi batching ƒë·ªÉ t·ªëi ∆∞u hi·ªáu su·∫•t.
        
        Args:
            user_inputs: Danh s√°ch c√°c tin nh·∫Øn t·ª´ ng∆∞·ªùi d√πng
            batch_size: K√≠ch th∆∞·ªõc batch cho m·ªói l·∫ßn x·ª≠ l√Ω
            
        Returns:
            Danh s√°ch c√°c ph·∫£n h·ªìi t∆∞∆°ng ·ª©ng
        """
        from functions import FUNCTION_SCHEMAS, execute_function_call
        import time
        
        results = []
        total_batches = (len(user_inputs) + batch_size - 1) // batch_size
        
        print(f"üîÑ X·ª≠ l√Ω {len(user_inputs)} requests v·ªõi {total_batches} batch(es) (k√≠ch th∆∞·ªõc batch: {batch_size})")
        
        for batch_idx in range(0, len(user_inputs), batch_size):
            batch_inputs = user_inputs[batch_idx:batch_idx + batch_size]
            current_batch = batch_idx // batch_size + 1
            
            print(f"üì¶ ƒêang x·ª≠ l√Ω batch {current_batch}/{total_batches} ({len(batch_inputs)} requests)")
            start_time = time.time()
            
            batch_results = []
            
            for i, user_input in enumerate(batch_inputs):
                try:
                    # T·∫°o b·∫£n sao conversation history cho m·ªói request trong batch
                    original_history = self.conversation_history.copy()
                    
                    # Add user message
                    temp_history = original_history + [{"role": "user", "content": user_input}]
                    
                    # Make API call
                    response = self.client.chat.completions.create(
                        model=self.model,
                        messages=temp_history,
                        tools=FUNCTION_SCHEMAS,
                        tool_choice="auto",
                        temperature=0.7,
                        max_tokens=1000
                    )
                    
                    message = response.choices[0].message
                    response_data = {
                        "input": user_input,
                        "content": message.content,
                        "tool_calls": [],
                        "function_results": [],
                        "total_tokens": response.usage.total_tokens if hasattr(response, 'usage') else 0,
                        "batch_index": current_batch,
                        "item_index": i + 1
                    }
                    
                    # Handle function calls n·∫øu c√≥
                    if message.tool_calls:
                        temp_history.append({
                            "role": "assistant",
                            "content": message.content,
                            "tool_calls": message.tool_calls
                        })
                        
                        for tool_call in message.tool_calls:
                            function_name = tool_call.function.name
                            function_args = json.loads(tool_call.function.arguments)
                            
                            # Execute function
                            function_result = execute_function_call(function_name, function_args)
                            
                            # Add function result
                            temp_history.append({
                                "role": "tool",
                                "tool_call_id": tool_call.id,
                                "content": json.dumps(function_result, ensure_ascii=False) if isinstance(function_result, dict) else str(function_result)
                            })
                            
                            response_data["tool_calls"].append({
                                "function": function_name,
                                "arguments": function_args,
                                "result": function_result
                            })
                        
                        # Get final response
                        final_response = self.client.chat.completions.create(
                            model=self.model,
                            messages=temp_history,
                            tools=FUNCTION_SCHEMAS,
                            tool_choice="auto",
                            temperature=0.7,
                            max_tokens=1000
                        )
                        
                        final_message = final_response.choices[0].message
                        response_data["content"] = final_message.content
                        response_data["total_tokens"] += final_response.usage.total_tokens if hasattr(final_response, 'usage') else 0
                    
                    batch_results.append(response_data)
                    
                except Exception as e:
                    error_response = {
                        "input": user_input,
                        "content": f"‚ùå L·ªói: {str(e)}",
                        "tool_calls": [],
                        "function_results": [],
                        "total_tokens": 0,
                        "batch_index": current_batch,
                        "item_index": i + 1,
                        "error": str(e)
                    }
                    batch_results.append(error_response)
            
            # Add delay gi·ªØa c√°c batch ƒë·ªÉ tr√°nh rate limiting
            batch_time = time.time() - start_time
            print(f"   ‚úÖ Ho√†n th√†nh batch {current_batch} trong {batch_time:.2f}s")
            
            if current_batch < total_batches:
                time.sleep(0.5)  # Ngh·ªâ 0.5s gi·ªØa c√°c batch
            
            results.extend(batch_results)
        
        # T√≠nh to√°n th·ªëng k√™ t·ªïng
        total_tokens = sum(r.get("total_tokens", 0) for r in results)
        successful_requests = len([r for r in results if "error" not in r])
        failed_requests = len(results) - successful_requests
        
        print(f"\nüìä K·∫æT QU·∫¢ BATCH PROCESSING:")
        print(f"   ‚Ä¢ T·ªïng requests: {len(results)}")
        print(f"   ‚Ä¢ Th√†nh c√¥ng: {successful_requests}")
        print(f"   ‚Ä¢ Th·∫•t b·∫°i: {failed_requests}")
        print(f"   ‚Ä¢ T·ªïng tokens: {total_tokens:,}")
        print(f"   ‚Ä¢ Trung b√¨nh tokens/request: {total_tokens/len(results):.1f}")
        
        return results
    
    def process_expense_batch(self, expenses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        X·ª≠ l√Ω batch c√°c chi ph√≠ ƒë·ªÉ t√≠nh to√°n v√† x√°c th·ª±c h√†ng lo·∫°t.
        
        Args:
            expenses: Danh s√°ch c√°c chi ph√≠ c·∫ßn x·ª≠ l√Ω
            
        Returns:
            Dictionary v·ªõi k·∫øt qu·∫£ x·ª≠ l√Ω t·ªïng h·ª£p
        """
        from functions import calculate_reimbursement, validate_expense, format_expense_summary
        
        print(f"üí∞ X·ª≠ l√Ω batch {len(expenses)} chi ph√≠...")
        
        # T√≠nh to√°n ho√†n tr·∫£ cho t·∫•t c·∫£ chi ph√≠
        reimbursement_result = calculate_reimbursement(expenses)
        
        # X√°c th·ª±c t·ª´ng chi ph√≠
        validation_results = []
        for i, expense in enumerate(expenses):
            validation = validate_expense(expense)
            validation["expense_index"] = i + 1
            validation["expense"] = expense
            validation_results.append(validation)
        
        # T·∫°o t√≥m t·∫Øt
        summary = format_expense_summary(expenses)
        
        # Ph√¢n t√≠ch k·∫øt qu·∫£
        valid_expenses = [v for v in validation_results if v["is_valid"]]
        invalid_expenses = [v for v in validation_results if not v["is_valid"]]
        expenses_with_warnings = [v for v in validation_results if v["warnings"]]
        
        batch_result = {
            "total_expenses": len(expenses),
            "reimbursement_calculation": reimbursement_result,
            "validation_results": validation_results,
            "summary": summary,
            "statistics": {
                "valid_count": len(valid_expenses),
                "invalid_count": len(invalid_expenses),
                "warnings_count": len(expenses_with_warnings),
                "total_submitted": reimbursement_result["total_submitted"],
                "total_reimbursed": reimbursement_result["total_reimbursed"],
                "savings": reimbursement_result["savings"]
            }
        }
        
        print(f"   ‚úÖ Ho√†n th√†nh: {len(valid_expenses)}/{len(expenses)} chi ph√≠ h·ª£p l·ªá")
        print(f"   üí∞ T·ªïng ho√†n tr·∫£: {reimbursement_result['total_reimbursed']:,.0f} VNƒê")
        
        return batch_result
        
    def get_response(self, user_input: str, max_retries: int = 3) -> Dict[str, Any]:
        """
        Nh·∫≠n ph·∫£n h·ªìi t·ª´ assistant v·ªõi h·ªó tr·ª£ g·ªçi h√†m (method g·ªëc cho single request).
        
        Args:
            user_input: Tin nh·∫Øn c·ªßa ng∆∞·ªùi d√πng
            max_retries: S·ªë l·∫ßn th·ª≠ l·∫°i t·ªëi ƒëa cho g·ªçi h√†m
            
        Returns:
            Dictionary v·ªõi chi ti·∫øt ph·∫£n h·ªìi
        """
        from functions import FUNCTION_SCHEMAS, execute_function_call
        
        try:
            # Add user message to history
            self.add_user_message(user_input)
            
            # Make API call with function calling enabled
            response = self.client.chat.completions.create(
                model=self.model,
                messages=self.conversation_history,
                tools=FUNCTION_SCHEMAS,
                tool_choice="auto",
                temperature=0.7,
                max_tokens=1000
            )
            
            message = response.choices[0].message
            response_data = {
                "content": message.content,
                "tool_calls": [],
                "function_results": [],
                "total_tokens": response.usage.total_tokens if hasattr(response, 'usage') else 0
            }
            
            # Handle function calls
            if message.tool_calls:
                self.conversation_history.append({
                    "role": "assistant",
                    "content": message.content,
                    "tool_calls": message.tool_calls
                })
                
                for tool_call in message.tool_calls:
                    function_name = tool_call.function.name
                    function_args = json.loads(tool_call.function.arguments)
                    
                    # Execute function
                    function_result = execute_function_call(function_name, function_args)
                    
                    # Add function result to conversation
                    self.conversation_history.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": json.dumps(function_result, ensure_ascii=False) if isinstance(function_result, dict) else str(function_result)
                    })
                    
                    response_data["tool_calls"].append({
                        "function": function_name,
                        "arguments": function_args,
                        "result": function_result
                    })
                
                # Get final response after function calls
                final_response = self.client.chat.completions.create(
                    model=self.model,
                    messages=self.conversation_history,
                    tools=FUNCTION_SCHEMAS,
                    tool_choice="auto",
                    temperature=0.7,
                    max_tokens=1000
                )
                
                final_message = final_response.choices[0].message
                response_data["content"] = final_message.content
                response_data["total_tokens"] += final_response.usage.total_tokens if hasattr(final_response, 'usage') else 0
                
                # Add final response to history
                self.add_assistant_message(final_message.content)
            else:
                # No function calls, just add the response
                self.add_assistant_message(message.content)
            
            return response_data
            
        except Exception as e:
            error_msg = f"‚ùå L·ªói: {str(e)}"
            self.add_assistant_message(error_msg)
            return {
                "content": error_msg,
                "tool_calls": [],
                "function_results": [],
                "total_tokens": 0,
                "error": str(e)
            }
        from functions import FUNCTION_SCHEMAS, execute_function_call
        
        try:
            # Add user message to history
            self.add_user_message(user_input)
            
            # Make API call with function calling enabled
            response = self.client.chat.completions.create(
                model=self.model,
                messages=self.conversation_history,
                tools=FUNCTION_SCHEMAS,
                tool_choice="auto",
                temperature=0.7,
                max_tokens=1000
            )
            
            message = response.choices[0].message
            response_data = {
                "content": message.content,
                "tool_calls": [],
                "function_results": [],
                "total_tokens": response.usage.total_tokens if hasattr(response, 'usage') else 0
            }
            
            # Handle function calls
            if message.tool_calls:
                self.conversation_history.append({
                    "role": "assistant",
                    "content": message.content,
                    "tool_calls": message.tool_calls
                })
                
                for tool_call in message.tool_calls:
                    function_name = tool_call.function.name
                    function_args = json.loads(tool_call.function.arguments)
                    
                    # Execute function
                    function_result = execute_function_call(function_name, function_args)
                    
                    # Add function result to conversation
                    self.conversation_history.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": json.dumps(function_result, ensure_ascii=False) if isinstance(function_result, dict) else str(function_result)
                    })
                    
                    response_data["tool_calls"].append({
                        "function": function_name,
                        "arguments": function_args,
                        "result": function_result
                    })
                
                # Get final response after function calls
                final_response = self.client.chat.completions.create(
                    model=self.model,
                    messages=self.conversation_history,
                    tools=FUNCTION_SCHEMAS,
                    tool_choice="auto",
                    temperature=0.7,
                    max_tokens=1000
                )
                
                final_message = final_response.choices[0].message
                response_data["content"] = final_message.content
                response_data["total_tokens"] += final_response.usage.total_tokens if hasattr(final_response, 'usage') else 0
                
                # Add final response to history
                self.add_assistant_message(final_message.content)
            else:
                # No function calls, just add the response
                self.add_assistant_message(message.content)
            
            return response_data
            
        except Exception as e:
            error_msg = f"‚ùå L·ªói: {str(e)}"
            self.add_assistant_message(error_msg)
            return {
                "content": error_msg,
                "tool_calls": [],
                "function_results": [],
                "total_tokens": 0,
                "error": str(e)
            }

def create_client():
    """T·∫°o v√† tr·∫£ v·ªÅ OpenAI client."""
    return OpenAI(
        base_url=os.getenv('OPENAI_BASE_URL'),
        api_key=os.getenv('OPENAI_API_KEY')
    )
