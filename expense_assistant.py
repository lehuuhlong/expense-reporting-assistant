# Tr·ª£ L√Ω B√°o C√°o Chi Ph√≠ - Module Ch√≠nh
"""
Tr·ª£ l√Ω b√°o c√°o chi ph√≠ ƒë∆∞·ª£c h·ªó tr·ª£ AI v·ªõi qu·∫£n l√Ω h·ªôi tho·∫°i,
g·ªçi h√†m v√† kh·∫£ nƒÉng h∆∞·ªõng d·∫´n ch√≠nh s√°ch.
"""

import os
import json
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import re
from dotenv import load_dotenv
from openai import OpenAI
from database import ExpenseDB

# Load environment variables
load_dotenv()

class ExpenseAssistant:
    """
    Tr·ª£ l√Ω b√°o c√°o chi ph√≠ ƒë∆∞·ª£c h·ªó tr·ª£ AI v·ªõi qu·∫£n l√Ω h·ªôi tho·∫°i,
    g·ªçi h√†m v√† kh·∫£ nƒÉng h∆∞·ªõng d·∫´n ch√≠nh s√°ch.
    """
    
    def __init__(self, client, model="GPT-4o-mini"):
        self.client = client
        self.model = model
        self.conversation_history = []
        self.user_context = {}  # Store user-specific context
        
        # Initialize ChromaDB connection
        self.db = ExpenseDB()
        
        # System prompt v·ªõi v√≠ d·ª• few-shot v√† ch√≠nh s√°ch c√¥ng ty
        self.system_prompt = """B·∫°n l√† Tr·ª£ L√Ω B√°o C√°o Chi Ph√≠ th√¥ng minh cho c√¥ng ty ch√∫ng t√¥i. 
Vai tr√≤ c·ªßa b·∫°n l√† gi√∫p nh√¢n vi√™n v·ªõi b√°o c√°o chi ph√≠, c√¢u h·ªèi ch√≠nh s√°ch v√† t√≠nh to√°n ho√†n ti·ªÅn.

TR√ÅCH NHI·ªÜM CH√çNH:
1. Tr·∫£ l·ªùi c√¢u h·ªèi ch√≠nh s√°ch chi ph√≠ m·ªôt c√°ch ch√≠nh x√°c
2. Gi√∫p x√°c th·ª±c v√† t√≠nh to√°n ho√†n ti·ªÅn chi ph√≠
3. H∆∞·ªõng d·∫´n ng∆∞·ªùi d√πng qua quy tr√¨nh n·ªôp chi ph√≠ ƒë√∫ng c√°ch
4. Cung c·∫•p ph·∫£n h·ªìi r√µ r√†ng, h·ªØu √≠ch v√† chuy√™n nghi·ªáp
5. T√¨m ki·∫øm th√¥ng tin t·ª´ knowledge base khi c·∫ßn thi·∫øt

T√ìM T·∫ÆT CH√çNH S√ÅCH CHI PH√ç C√îNG TY:
- H√≥a ƒë∆°n ƒë∆∞·ª£c y√™u c·∫ßu cho chi ph√≠ tr√™n 500.000 VNƒê
- Gi·ªõi h·∫°n ƒÉn u·ªëng: 1.000.000 VNƒê/ng√†y trong n∆∞·ªõc, 1.500.000 VNƒê/ng√†y qu·ªëc t·∫ø
- ƒêi l·∫°i c·∫ßn ph√™ duy·ªát tr∆∞·ªõc
- VƒÉn ph√≤ng ph·∫©m: gi·ªõi h·∫°n 2.000.000 VNƒê/th√°ng
- B√°o c√°o chi ph√≠ ph·∫£i n·ªôp trong v√≤ng 30 ng√†y
- T·ª∑ l·ªá xƒÉng xe: 3.000 VNƒê/km

H∆Ø·ªöNG D·∫™N:
- Khi ng∆∞·ªùi d√πng h·ªèi v·ªÅ ch√≠nh s√°ch, h√£y t√¨m ki·∫øm trong knowledge base tr∆∞·ªõc
- S·ª≠ d·ª•ng th√¥ng tin t·ª´ ChromaDB ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi ch√≠nh x√°c
- Lu√¥n cung c·∫•p th√¥ng tin c·∫≠p nh·∫≠t v√† ƒë·∫ßy ƒë·ªß

PHONG C√ÅCH H·ªòI THO·∫†I:
- Th√¢n thi·ªán, chuy√™n nghi·ªáp v√† h·ªØu √≠ch
- S·ª≠ d·ª•ng emojis ph√π h·ª£p ƒë·ªÉ l√†m cho cu·ªôc tr√≤ chuy·ªán th√∫ v·ªã h∆°n
- ∆Øu ti√™n cung c·∫•p th√¥ng tin ch√≠nh x√°c v√† c√≥ th·ªÉ h√†nh ƒë·ªông ƒë∆∞·ª£c

V√ç D·ª§ H·ªòI THO·∫†I:

Ng∆∞·ªùi d√πng: "T√¥i c√≥ chi ph√≠ ƒÉn u·ªëng 900.000 VNƒê, c√≥ ƒë∆∞·ª£c ho√†n kh√¥ng?"
Tr·ª£ l√Ω: "üçΩÔ∏è Chi ph√≠ ƒÉn u·ªëng 900.000 VNƒê c·ªßa b·∫°n n·∫±m trong gi·ªõi h·∫°n 1.000.000 VNƒê/ng√†y! ‚úÖ C√≥ th·ªÉ ƒë∆∞·ª£c ho√†n ƒë·∫ßy ƒë·ªß. B·∫°n c√≥ h√≥a ƒë∆°n ch∆∞a? üßæ"

Nh·ªõ lu√¥n ki·ªÉm tra knowledge base tr∆∞·ªõc khi tr·∫£ l·ªùi v·ªÅ ch√≠nh s√°ch!"""

        # Initialize conversation v·ªõi system prompt
        self.conversation_history = [{"role": "system", "content": self.system_prompt}]
    
    def add_user_message(self, content: str):
        """Th√™m tin nh·∫Øn ng∆∞·ªùi d√πng v√†o l·ªãch s·ª≠ h·ªôi tho·∫°i."""
        self.conversation_history.append({"role": "user", "content": content})
    
    def add_assistant_message(self, content: str):
        """Th√™m tin nh·∫Øn tr·ª£ l√Ω v√†o l·ªãch s·ª≠ h·ªôi tho·∫°i."""
        self.conversation_history.append({"role": "assistant", "content": content})
    
    def get_conversation_summary(self) -> Dict[str, Any]:
        """L·∫•y t√≥m t·∫Øt cu·ªôc h·ªôi tho·∫°i hi·ªán t·∫°i."""
        user_messages = [msg for msg in self.conversation_history if msg["role"] == "user"]
        assistant_messages = [msg for msg in self.conversation_history if msg["role"] == "assistant"]
        
        total_tokens = sum(len(msg["content"].split()) for msg in self.conversation_history)
        
        return {
            "total_exchanges": len(user_messages),
            "user_messages": len(user_messages),
            "assistant_messages": len(assistant_messages),
            "total_messages": len(self.conversation_history),
            "estimated_tokens": total_tokens
        }
    
    def search_knowledge_base(self, query: str) -> Dict[str, Any]:
        """
        T√¨m ki·∫øm th√¥ng tin t·ª´ ChromaDB knowledge base.
        
        Args:
            query: C√¢u h·ªèi ho·∫∑c t·ª´ kh√≥a t√¨m ki·∫øm
            
        Returns:
            Dictionary ch·ª©a k·∫øt qu·∫£ t√¨m ki·∫øm t·ª´ c√°c collections
        """
        try:
            results = {
                "policies": self.db.search_policies(query, limit=3),
                "query": query,
                "found": False
            }
            
            # Ki·ªÉm tra xem c√≥ t√¨m th·∫•y k·∫øt qu·∫£ kh√¥ng
            if results["policies"]:
                results["found"] = True
                
            return results
        except Exception as e:
            return {
                "policies": [],
                "query": query,
                "found": False,
                "error": str(e)
            }
    
    def clear_conversation(self):
        """X√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i nh∆∞ng gi·ªØ system prompt."""
        self.conversation_history = [self.conversation_history[0]]  # Gi·ªØ system prompt
        self.user_context = {}
    
    def get_response(self, user_input: str, max_retries: int = 3) -> Dict[str, Any]:
        """
        Nh·∫≠n ph·∫£n h·ªìi t·ª´ assistant v·ªõi h·ªó tr·ª£ g·ªçi h√†m v√† t√¨m ki·∫øm knowledge base.
        
        Args:
            user_input: Tin nh·∫Øn c·ªßa ng∆∞·ªùi d√πng
            max_retries: S·ªë l·∫ßn th·ª≠ l·∫°i t·ªëi ƒëa cho g·ªçi h√†m
            
        Returns:
            Dictionary v·ªõi chi ti·∫øt ph·∫£n h·ªìi
        """
        from functions import FUNCTION_SCHEMAS, execute_function_call
        
        # T·ª± ƒë·ªông t√¨m ki·∫øm knowledge base cho c√°c c√¢u h·ªèi ch√≠nh s√°ch
        knowledge_base_keywords = [
            'ch√≠nh s√°ch', 'policy', 'quy ƒë·ªãnh', 'gi·ªõi h·∫°n', 'limit', 
            'h√≥a ƒë∆°n', 'receipt', 'y√™u c·∫ßu', 'requirement', 'quy tr√¨nh',
            'h·∫°n', 'deadline', 'n·ªôp', 'submit'
        ]
        
        should_search_kb = any(keyword in user_input.lower() for keyword in knowledge_base_keywords)
        kb_results = {}
        
        enhanced_input = user_input
        if should_search_kb:
            kb_results = self.search_knowledge_base(user_input)
            if kb_results["found"]:
                kb_context = f"\n\nüîç Th√¥ng tin t·ª´ knowledge base:\n"
                for policy in kb_results["policies"]:
                    kb_context += f"‚Ä¢ {policy}\n"
                
                # Th√™m context v√†o tin nh·∫Øn c·ªßa user
                enhanced_input = f"{user_input}{kb_context}"
        
        try:
            # Add enhanced user message to history
            self.add_user_message(enhanced_input)
            
            # Make API call with function calling enabled
            response = self.client.chat.completions.create(
                model=self.model,
                messages=self.conversation_history,
                tools=FUNCTION_SCHEMAS,
                tool_choice="auto",
                temperature=0.7,
                max_tokens=1000
            )
            
            message = response.choices[0].message
            response_data = {
                "content": message.content,
                "tool_calls": [],
                "function_results": [],
                "total_tokens": response.usage.total_tokens if hasattr(response, 'usage') else 0,
                "knowledge_base_used": should_search_kb and kb_results.get("found", False)
            }
            
            # Handle function calls
            if message.tool_calls:
                self.conversation_history.append({
                    "role": "assistant",
                    "content": message.content,
                    "tool_calls": message.tool_calls
                })
                
                for tool_call in message.tool_calls:
                    function_name = tool_call.function.name
                    function_args = json.loads(tool_call.function.arguments)
                    
                    # Execute function
                    function_result = execute_function_call(function_name, function_args)
                    
                    # Add function result to conversation
                    self.conversation_history.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": json.dumps(function_result, ensure_ascii=False) if isinstance(function_result, dict) else str(function_result)
                    })
                    
                    response_data["tool_calls"].append({
                        "function": function_name,
                        "arguments": function_args,
                        "result": function_result
                    })
                
                # Get final response after function calls
                final_response = self.client.chat.completions.create(
                    model=self.model,
                    messages=self.conversation_history,
                    tools=FUNCTION_SCHEMAS,
                    tool_choice="auto",
                    temperature=0.7,
                    max_tokens=1000
                )
                
                final_message = final_response.choices[0].message
                response_data["content"] = final_message.content
                response_data["total_tokens"] += final_response.usage.total_tokens if hasattr(final_response, 'usage') else 0
                
                # Add final response to history
                self.add_assistant_message(final_message.content)
            else:
                # No function calls, just add the response
                self.add_assistant_message(message.content)
            
            return response_data
            
        except Exception as e:
            error_msg = f"‚ùå L·ªói: {str(e)}"
            self.add_assistant_message(error_msg)
            return {
                "content": error_msg,
                "tool_calls": [],
                "function_results": [],
                "total_tokens": 0,
                "error": str(e),
                "knowledge_base_used": False
            }
    
    def process_batch_requests(self, user_inputs: List[str], batch_size: int = 5) -> List[Dict[str, Any]]:
        """
        X·ª≠ l√Ω nhi·ªÅu request c√πng l√∫c v·ªõi batching ƒë·ªÉ t·ªëi ∆∞u hi·ªáu su·∫•t.
        
        Args:
            user_inputs: Danh s√°ch c√°c tin nh·∫Øn t·ª´ ng∆∞·ªùi d√πng
            batch_size: K√≠ch th∆∞·ªõc batch cho m·ªói l·∫ßn x·ª≠ l√Ω
            
        Returns:
            Danh s√°ch c√°c ph·∫£n h·ªìi t∆∞∆°ng ·ª©ng
        """
        from functions import FUNCTION_SCHEMAS, execute_function_call
        import time
        
        results = []
        total_batches = (len(user_inputs) + batch_size - 1) // batch_size
        
        print(f"üîÑ X·ª≠ l√Ω {len(user_inputs)} requests v·ªõi {total_batches} batch(es) (k√≠ch th∆∞·ªõc batch: {batch_size})")
        
        for batch_idx in range(0, len(user_inputs), batch_size):
            batch_inputs = user_inputs[batch_idx:batch_idx + batch_size]
            current_batch = batch_idx // batch_size + 1
            
            print(f"üì¶ ƒêang x·ª≠ l√Ω batch {current_batch}/{total_batches} ({len(batch_inputs)} requests)")
            start_time = time.time()
            
            batch_results = []
            
            for i, user_input in enumerate(batch_inputs):
                try:
                    # T·∫°o b·∫£n sao conversation history cho m·ªói request trong batch
                    original_history = self.conversation_history.copy()
                    
                    # Add user message
                    temp_history = original_history + [{"role": "user", "content": user_input}]
                    
                    # Make API call
                    response = self.client.chat.completions.create(
                        model=self.model,
                        messages=temp_history,
                        tools=FUNCTION_SCHEMAS,
                        tool_choice="auto",
                        temperature=0.7,
                        max_tokens=1000
                    )
                    
                    message = response.choices[0].message
                    response_data = {
                        "input": user_input,
                        "content": message.content,
                        "tool_calls": [],
                        "function_results": [],
                        "total_tokens": response.usage.total_tokens if hasattr(response, 'usage') else 0,
                        "batch_index": current_batch,
                        "item_index": i + 1
                    }
                    
                    # Handle function calls n·∫øu c√≥
                    if message.tool_calls:
                        temp_history.append({
                            "role": "assistant",
                            "content": message.content,
                            "tool_calls": message.tool_calls
                        })
                        
                        for tool_call in message.tool_calls:
                            function_name = tool_call.function.name
                            function_args = json.loads(tool_call.function.arguments)
                            
                            # Execute function
                            function_result = execute_function_call(function_name, function_args)
                            
                            # Add function result
                            temp_history.append({
                                "role": "tool",
                                "tool_call_id": tool_call.id,
                                "content": json.dumps(function_result, ensure_ascii=False) if isinstance(function_result, dict) else str(function_result)
                            })
                            
                            response_data["tool_calls"].append({
                                "function": function_name,
                                "arguments": function_args,
                                "result": function_result
                            })
                        
                        # Get final response
                        final_response = self.client.chat.completions.create(
                            model=self.model,
                            messages=temp_history,
                            tools=FUNCTION_SCHEMAS,
                            tool_choice="auto",
                            temperature=0.7,
                            max_tokens=1000
                        )
                        
                        final_message = final_response.choices[0].message
                        response_data["content"] = final_message.content
                        response_data["total_tokens"] += final_response.usage.total_tokens if hasattr(final_response, 'usage') else 0
                    
                    batch_results.append(response_data)
                    
                except Exception as e:
                    error_response = {
                        "input": user_input,
                        "content": f"‚ùå L·ªói: {str(e)}",
                        "tool_calls": [],
                        "function_results": [],
                        "total_tokens": 0,
                        "batch_index": current_batch,
                        "item_index": i + 1,
                        "error": str(e)
                    }
                    batch_results.append(error_response)
            
            # Add delay gi·ªØa c√°c batch ƒë·ªÉ tr√°nh rate limiting
            batch_time = time.time() - start_time
            print(f"   ‚úÖ Ho√†n th√†nh batch {current_batch} trong {batch_time:.2f}s")
            
            if current_batch < total_batches:
                time.sleep(0.5)  # Ngh·ªâ 0.5s gi·ªØa c√°c batch
            
            results.extend(batch_results)
        
        # T√≠nh to√°n th·ªëng k√™ t·ªïng
        total_tokens = sum(r.get("total_tokens", 0) for r in results)
        successful_requests = len([r for r in results if "error" not in r])
        failed_requests = len(results) - successful_requests
        
        print(f"\nüìä K·∫æT QU·∫¢ BATCH PROCESSING:")
        print(f"   ‚Ä¢ T·ªïng requests: {len(results)}")
        print(f"   ‚Ä¢ Th√†nh c√¥ng: {successful_requests}")
        print(f"   ‚Ä¢ Th·∫•t b·∫°i: {failed_requests}")
        print(f"   ‚Ä¢ T·ªïng tokens: {total_tokens:,}")
        print(f"   ‚Ä¢ Trung b√¨nh tokens/request: {total_tokens/len(results):.1f}")
        
        return results
    
    def process_expense_batch(self, expenses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        X·ª≠ l√Ω batch c√°c chi ph√≠ ƒë·ªÉ t√≠nh to√°n v√† x√°c th·ª±c h√†ng lo·∫°t.
        
        Args:
            expenses: Danh s√°ch c√°c chi ph√≠ c·∫ßn x·ª≠ l√Ω
            
        Returns:
            Dictionary v·ªõi k·∫øt qu·∫£ x·ª≠ l√Ω t·ªïng h·ª£p
        """
        from functions import calculate_reimbursement, validate_expense, format_expense_summary
        
        print(f"üí∞ X·ª≠ l√Ω batch {len(expenses)} chi ph√≠...")
        
        # T√≠nh to√°n ho√†n tr·∫£ cho t·∫•t c·∫£ chi ph√≠
        reimbursement_result = calculate_reimbursement(expenses)
        
        # X√°c th·ª±c t·ª´ng chi ph√≠
        validation_results = []
        for i, expense in enumerate(expenses):
            validation = validate_expense(expense)
            validation["expense_index"] = i + 1
            validation["expense"] = expense
            validation_results.append(validation)
        
        # T·∫°o t√≥m t·∫Øt
        summary = format_expense_summary(expenses)
        
        # Ph√¢n t√≠ch k·∫øt qu·∫£
        valid_expenses = [v for v in validation_results if v["is_valid"]]
        invalid_expenses = [v for v in validation_results if not v["is_valid"]]
        expenses_with_warnings = [v for v in validation_results if v["warnings"]]
        
        batch_result = {
            "total_expenses": len(expenses),
            "reimbursement_calculation": reimbursement_result,
            "validation_results": validation_results,
            "summary": summary,
            "statistics": {
                "valid_count": len(valid_expenses),
                "invalid_count": len(invalid_expenses),
                "warnings_count": len(expenses_with_warnings),
                "total_submitted": reimbursement_result["total_submitted"],
                "total_reimbursed": reimbursement_result["total_reimbursed"],
                "savings": reimbursement_result["savings"]
            }
        }
        
        print(f"   ‚úÖ Ho√†n th√†nh: {len(valid_expenses)}/{len(expenses)} chi ph√≠ h·ª£p l·ªá")
        print(f"   üí∞ T·ªïng ho√†n tr·∫£: {reimbursement_result['total_reimbursed']:,.0f} VNƒê")
        
        return batch_result

def create_client():
    """T·∫°o v√† tr·∫£ v·ªÅ OpenAI client."""
    return OpenAI(
        base_url=os.getenv('OPENAI_BASE_URL'),
        api_key=os.getenv('OPENAI_API_KEY')
    )
